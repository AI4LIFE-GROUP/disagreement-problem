{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr, rankdata\n",
    "import itertools\n",
    "from scipy.special import comb\n",
    "import collections\n",
    "import scipy\n",
    "from scipy.stats import sem\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### \n",
    "\n",
    "accuracy = 90.6\n",
    "vocab_size = 95811\n",
    "avg_len = 43.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.7575"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "43.03*0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit by the length of the sentence\n",
    "def convert_to_dict(exs, lime_exs):\n",
    "    return {exs[i][0][:len(lime_exs[i][0])] : exs[i][1].numpy()[:len(lime_exs[i][1])] for i in range(len(lime_exs))}\n",
    "\n",
    "def convert_to_dict_lime(exs):\n",
    "    return {exs[i][0] : exs[i][1].numpy() for i in range(len(exs))}\n",
    "\n",
    "with open(\"lime_explanation2.pkl\", \"rb\") as limes, open(\"shap_explanation_cpu.pkl\", \"rb\") as shaps, open(\"ig_explanation_cpu.pkl\", \"rb\") as igs, open(\"g_explanation_cpu.pkl\", \"rb\") as gs, open(\"gi_explanation_cpu.pkl\", \"rb\") as gis, open(\"sg_explanation_cpu.pkl\", \"rb\") as sgs:\n",
    "    lime = pickle.load(limes)\n",
    "    shap_ex = convert_to_dict(pickle.load(shaps), lime)\n",
    "    igs_ex = convert_to_dict(pickle.load(igs), lime)\n",
    "    gs_ex = convert_to_dict(pickle.load(gs), lime)\n",
    "    sgs_ex = convert_to_dict(pickle.load(sgs), lime)\n",
    "    gis_ex = convert_to_dict(pickle.load(gis), lime)\n",
    "    limes_ex = convert_to_dict_lime(lime)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks\n",
    "len(shap_ex) == len(igs_ex) == len(gs_ex) == len(sgs_ex) == len(gis_ex) == len(limes_ex)\n",
    "\n",
    "shap_ex.keys() == igs_ex.keys() == gs_ex.keys() == sgs_ex.keys() == gis_ex.keys() == limes_ex.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping a list copy of explanation for faster processing\n",
    "shap_ex_l = list(collections.OrderedDict(sorted(shap_ex.items())).values())\n",
    "igs_ex_l = list(collections.OrderedDict(sorted(igs_ex.items())).values())\n",
    "gs_ex_l = list(collections.OrderedDict(sorted(gs_ex.items())).values())\n",
    "sgs_ex_l = list(collections.OrderedDict(sorted(sgs_ex.items())).values())\n",
    "gis_ex_l = list(collections.OrderedDict(sorted(gis_ex.items())).values())\n",
    "limes_ex_l = list(collections.OrderedDict(sorted(limes_ex.items())).values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering some points with the same attributions for all tokens (5 out of 7600 samples). \n",
    "outliers_inds = []\n",
    "for i in range(len(limes_ex_l)):\n",
    "    if len(set([abs(i) for i in limes_ex_l[i]])) == 1:\n",
    "        outliers_inds.append(i)\n",
    "\n",
    "shap_ex_l = [shap_ex_l[i] for i in range(len(shap_ex_l)) if i not in outliers_inds]\n",
    "igs_ex_l = [igs_ex_l[i] for i in range(len(igs_ex_l)) if i not in outliers_inds]\n",
    "limes_ex_l = [limes_ex_l[i] for i in range(len(limes_ex_l)) if i not in outliers_inds]\n",
    "gis_ex_l = [gis_ex_l[i] for i in range(len(gis_ex_l)) if i not in outliers_inds]\n",
    "sgs_ex_l = [sgs_ex_l[i] for i in range(len(sgs_ex_l)) if i not in outliers_inds]\n",
    "gs_ex_l = [gs_ex_l[i] for i in range(len(gs_ex_l)) if i not in outliers_inds]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank correlation\n",
    "\n",
    "def rankcorr(attrA, attrB):\n",
    "    '''\n",
    "    attrA: np.array, n x p\n",
    "    attrB: np.array, n x p\n",
    "    '''\n",
    "    corrs = []\n",
    "    \n",
    "    for row in range(len(attrA)) : \n",
    "        all_feat_ranksA = rankdata(-np.abs(attrA[row]), method='dense')\n",
    "        all_feat_ranksB = rankdata(-np.abs(attrB[row]), method='dense')\n",
    "        rho, _ = pearsonr(all_feat_ranksA, all_feat_ranksB) \n",
    "        corrs.append(rho)\n",
    "    return np.array(corrs), np.mean(corrs) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_comp(attrA, attrB):\n",
    "    \n",
    "    '''\n",
    "    inputs\n",
    "    attrA: np.array, n x p\n",
    "    attrB: np.array, n x p\n",
    "    \n",
    "    outputs:\n",
    "    pairwise_distr: 1D numpy array (dimensions=(n,)) of pairwise comparison agreement for each data point\n",
    "    pairwise_avg: mean of pairwise_distr\n",
    "    '''\n",
    "    pairwise_distr = []\n",
    "    for i in range(len(attrA)):\n",
    "        n_feat = len(attrA[i]) \n",
    "    \n",
    "        #rank of all features --> manually calculate rankings (instead of using 0, 1, ..., k ranking based on argsort output) to account for ties\n",
    "        all_feat_ranksA = rankdata(-np.abs(attrA[i]), method='dense') #rankdata gives rank1 for smallest # --> we want rank1 for largest # (aka # with largest magnitude)\n",
    "        all_feat_ranksB = rankdata(-np.abs(attrB[i]), method='dense') \n",
    "        \n",
    "        feat_pairs_w_same_rel_rankings = 0 \n",
    "\n",
    "        for feat1, feat2 in itertools.combinations_with_replacement(range(n_feat), 2):\n",
    "            if feat1 != feat2: \n",
    "                rel_rankingA = all_feat_ranksA[feat1] <= all_feat_ranksA[feat2]\n",
    "                rel_rankingB = all_feat_ranksB[feat1] <= all_feat_ranksB[feat2]\n",
    "                feat_pairs_w_same_rel_rankings += rel_rankingA == rel_rankingB\n",
    "\n",
    "        pairwise_distr.append(feat_pairs_w_same_rel_rankings/comb(n_feat, 2)) # = feat_pairs_w_same_rel_rankings/comb(n_feat, 2)\n",
    "    \n",
    "    \n",
    "    return pairwise_distr, np.mean(pairwise_distr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agreement_fraction(attrA, attrB, k, metric_type=['overlap', 'rank', 'sign', 'ranksign']):\n",
    "    \n",
    "    #id of top-k features\n",
    "    topk_idA = []\n",
    "    topk_idB = []\n",
    "    for i,_ in enumerate(attrA): \n",
    "        topk_idA.append(np.argsort(-np.abs(attrA[i]))[0:k])\n",
    "        topk_idB.append(np.argsort(-np.abs(attrB[i]))[0:k])\n",
    "\n",
    "    all_feat_ranksA = []\n",
    "    all_feat_ranksB = []\n",
    "    topk_ranksA = []\n",
    "    topk_ranksB = []\n",
    "    topk_signA = []\n",
    "    topk_signB = []\n",
    "    for i,_ in enumerate(attrA):\n",
    "        all_feat_ranksA.append(rankdata(-np.abs(attrA[i]), method='dense')) #rankdata gives rank1 for smallest # --> we want rank1 for largest # (aka # with largest magnitude)\n",
    "        all_feat_ranksB.append(rankdata(-np.abs(attrB[i]), method='dense')) \n",
    "    \n",
    "        topk_ranksA.append(np.take_along_axis(all_feat_ranksA[-1], topk_idA[i], axis = 0)) \n",
    "        topk_ranksB.append(np.take_along_axis(all_feat_ranksB[-1], topk_idB[i], axis = 0)) \n",
    "        \n",
    "        topk_signA.append(np.take_along_axis(np.sign(attrA[i]), topk_idA[i], axis = 0))\n",
    "        topk_signB.append(np.take_along_axis(np.sign(attrB[i]), topk_idB[i], axis = 0))\n",
    "\n",
    "\n",
    "    if metric_type=='overlap':\n",
    "        topk_setsA = [set(row) for row in topk_idA]\n",
    "        topk_setsB = [set(row) for row in topk_idB]\n",
    "        metric_distr = np.array([len(setA.intersection(setB))/k for setA, setB in zip(topk_setsA, topk_setsB)])\n",
    "\n",
    "    #rank agreement\n",
    "    elif metric_type=='rank':    \n",
    "        topk_idA_df = pd.DataFrame(topk_idA).applymap(str) #id\n",
    "        topk_idB_df = pd.DataFrame(topk_idB).applymap(str)\n",
    "        topk_ranksA_df = pd.DataFrame(topk_ranksA).applymap(str) #rank (accounting for ties)\n",
    "        topk_ranksB_df = pd.DataFrame(topk_ranksB).applymap(str)\n",
    "        topk_id_ranksA_df = ('feat' + topk_idA_df) + ('rank' + topk_ranksA_df)\n",
    "        topk_id_ranksB_df = ('feat' + topk_idB_df) + ('rank' + topk_ranksB_df)\n",
    "        metric_distr = (topk_id_ranksA_df == topk_id_ranksB_df).sum(axis=1).to_numpy()/k\n",
    "\n",
    "    #sign agreement\n",
    "    elif metric_type=='sign':           \n",
    "        topk_idA_df = pd.DataFrame(topk_idA).applymap(str) #id (contains rank info --> order of features in columns)\n",
    "        topk_idB_df = pd.DataFrame(topk_idB).applymap(str)\n",
    "        topk_signA_df = pd.DataFrame(topk_signA).applymap(str) #sign\n",
    "        topk_signB_df = pd.DataFrame(topk_signB).applymap(str)\n",
    "        topk_id_signA_df = ('feat' + topk_idA_df) + ('sign' + topk_signA_df) #id + sign (contains rank info --> order of features in columns)\n",
    "        topk_id_signB_df = ('feat' + topk_idB_df) + ('sign' + topk_signB_df)\n",
    "        topk_id_signA_sets = [set(row) for row in topk_id_signA_df.to_numpy()] #id + sign (remove order info --> by converting to sets)\n",
    "        topk_id_signB_sets = [set(row) for row in topk_id_signB_df.to_numpy()]\n",
    "        metric_distr = np.array([len(setA.intersection(setB))/k for setA, setB in zip(topk_id_signA_sets, topk_id_signB_sets)])\n",
    "  \n",
    "    #rank and sign agreement\n",
    "    elif metric_type=='ranksign':    \n",
    "        topk_idA_df = pd.DataFrame(topk_idA).applymap(str) #id\n",
    "        topk_idB_df = pd.DataFrame(topk_idB).applymap(str)\n",
    "        topk_ranksA_df = pd.DataFrame(topk_ranksA).applymap(str) #rank (accounting for ties)\n",
    "        topk_ranksB_df = pd.DataFrame(topk_ranksB).applymap(str)\n",
    "        topk_signA_df = pd.DataFrame(topk_signA).applymap(str) #sign\n",
    "        topk_signB_df = pd.DataFrame(topk_signB).applymap(str)\n",
    "        topk_id_ranks_signA_df = ('feat' + topk_idA_df) + ('rank' + topk_ranksA_df) + ('sign' + topk_signA_df)\n",
    "        topk_id_ranks_signB_df = ('feat' + topk_idB_df) + ('rank' + topk_ranksB_df) + ('sign' + topk_signB_df)\n",
    "        metric_distr = (topk_id_ranks_signA_df == topk_id_ranks_signB_df).sum(axis=1).to_numpy()/k\n",
    "        \n",
    "    return metric_distr, np.mean(metric_distr)\n",
    "    \n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_all_methodpairs(expl_methods_names, expl_methods_attrs, k,\n",
    "                           metric_type=['rankcorr', 'pairwise', 'overlap', 'rank', 'sign', 'ranksign']):\n",
    "    '''\n",
    "    inputs\n",
    "    expl_methods_names: dictionary with string names of explanation methods (keys=method_index, values=method_name_string)\n",
    "    expl_methods_attrs: dictionary of attributions (keys=method_index, values=attributions for a model)\n",
    "    \n",
    "    outputs\n",
    "    method_pairs_distr: dictionary with distribution of rank correlation for each method pair (keys=method pair, values=1D array of metric values for each datapoint)\n",
    "    method_pairs_avg: array with average rank correlation for each pair of methods\n",
    "    '''\n",
    "    #initiate \n",
    "    method_pairs_distr = {} #dict to store distribution of metric for each method pair\n",
    "    num_expl_methods = len(expl_methods_attrs) #array to store average of metric for each method pair\n",
    "    method_pairs_avg = np.zeros([num_expl_methods, num_expl_methods])\n",
    "    method_pairs_sem = np.zeros([num_expl_methods, num_expl_methods]) #array to store sem of metric average for each method pair\n",
    "    \n",
    "    #fill corr_distr and avg_corr_matrix \n",
    "    for a, b in itertools.combinations_with_replacement(range(num_expl_methods), 2):\n",
    "        \n",
    "        #calculate metric for method pair\n",
    "        if metric_type=='rankcorr': \n",
    "            metric_distr, metric_avg = rankcorr(attrA=expl_methods_attrs[a], attrB=expl_methods_attrs[b])\n",
    "\n",
    "        elif metric_type=='pairwise':\n",
    "            metric_distr, metric_avg = pairwise_comp(attrA=expl_methods_attrs[a], attrB=expl_methods_attrs[b])\n",
    "\n",
    "        elif metric_type in ['overlap', 'rank', 'sign', 'ranksign']: \n",
    "            metric_distr, metric_avg = agreement_fraction(attrA=expl_methods_attrs[a], attrB=expl_methods_attrs[b], k=k, metric_type=metric_type)\n",
    "\n",
    "        #store metric distribution in dictionary\n",
    "        if a != b:\n",
    "            method_pairs_distr[f'{expl_methods_names[a]} vs. {expl_methods_names[b]}'] = metric_distr\n",
    "        \n",
    "        #store metric average in array\n",
    "        method_pairs_avg[a, b] = metric_avg\n",
    "        method_pairs_avg[b, a] = metric_avg\n",
    "        #store metric sem in array\n",
    "        method_pairs_sem[a, b] = sem(metric_distr)\n",
    "        method_pairs_sem[b, a] = sem(metric_distr)\n",
    "        \n",
    "    return method_pairs_distr, method_pairs_avg, method_pairs_sem\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot boxplots, function\n",
    "def boxplot_metric_distr(method_pairs_distr, plot_path, metric_type=['rankcorr', 'pairwise', 'overlap', 'rank', 'sign', 'ranksign']):\n",
    "    '''\n",
    "    method_pairs_distr: output of metric_all_methodpairs()\n",
    "    '''\n",
    "    #boxplot\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(5.5, 5.5))\n",
    "    axes.set(ylim=(-1.1, 1.1))\n",
    "    bp = sns.boxplot(data=list(method_pairs_distr.values()), color='cornflowerblue', ax=axes)\n",
    "    bp.set_xticklabels(list(method_pairs_distr.keys()), rotation=90)\n",
    "    \n",
    "    metric_string = {'rankcorr': 'Rank correlation', \n",
    "                     'pairwise': 'Pairwise comparison agreement', \n",
    "                     'overlap': 'Overlap agreement', \n",
    "                     'rank': 'Rank agreement', \n",
    "                     'sign': 'Sign agreement', \n",
    "                     'ranksign': 'Signed rank agreement'}\n",
    "    bp.set(xlabel='Method pair', ylabel=metric_string[metric_type]);\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(plot_path, facecolor='white', transparent=False, bbox_inches='tight', dpi=1200)\n",
    "    \n",
    "\n",
    "#plot boxplots, function\n",
    "def boxplot_metric_distr(method_pairs_distr, plot_path, k_title, metric_type=['rankcorr', 'pairwise', 'overlap', 'rank', 'sign', 'ranksign']):\n",
    "    '''\n",
    "    method_pairs_distr: output of metric_all_methodpairs()\n",
    "    '''\n",
    "    #boxplot\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(5.5, 5.5))\n",
    "    axes.set(ylim=(-1.1 if metric_type=='rankcorr' else -0.1, 1.1))\n",
    "    \n",
    "    bp = sns.boxplot(data=list(method_pairs_distr.values()), color='cornflowerblue', ax=axes)\n",
    "    bp.set_xticklabels(list(method_pairs_distr.keys()), rotation=90)\n",
    "\n",
    "    metric_string = {'rankcorr': 'Rank correlation', \n",
    "                     'pairwise': 'Pairwise rank agreement', \n",
    "                     'overlap': 'Feature agreement', \n",
    "                     'rank': 'Rank agreement', \n",
    "                     'sign': 'Sign agreement', \n",
    "                     'ranksign': 'Signed rank agreement'}\n",
    "\n",
    "    if metric_type in ['rankcorr', 'pairwise']:\n",
    "        plot_title=metric_string[metric_type]\n",
    "    else:\n",
    "        print(metric_string, metric_type, k_title)\n",
    "        print(metric_string[metric_type])\n",
    "        plot_title=f'{metric_string[metric_type]} (k = {k_title})'\n",
    "\n",
    "    bp.set(xlabel='Pair of explanation methods', ylabel=metric_string[metric_type], title=plot_title)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(plot_path, facecolor='white', transparent=False, bbox_inches='tight', dpi=1200)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_metric_avg(method_pairs_avg, method_pairs_sem, plot_path, \n",
    "                       metric_type=['rankcorr', 'pairwise', 'overlap', 'rank', 'sign', 'ranksign'],\n",
    "                       labels=['LIME', 'KernelSHAP', 'Grad', 'Grad*Input', 'IntGrad', 'SmoothGRAD']):\n",
    "    \n",
    "    '''\n",
    "    method_pairs_avg: output of metric_all_methodpairs()\n",
    "    '''\n",
    "    \n",
    "    cmap = sns.color_palette('vlag', as_cmap=True) #diverging colormap\n",
    "    \n",
    "    #heatmap\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    sns.heatmap(method_pairs_avg, cmap=cmap, #mask=mask, \n",
    "                vmin=-1, vmax=1, center=0,\n",
    "                xticklabels=labels, yticklabels=labels, annot=True, fmt='.4f',\n",
    "                square=True, linewidths=.5, cbar_kws={'shrink': 0.995}) #annot_kws={'fontsize':'large'}\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    metric_string = {'rankcorr': 'Rank correlation', \n",
    "                     'pairwise': 'Pairwise comparison agreement', \n",
    "                     'overlap': 'Overlap agreement', \n",
    "                     'rank': 'Rank agreement', \n",
    "                     'sign': 'Sign agreement', \n",
    "                     'ranksign': 'Signed rank agreement'}\n",
    "    plt.title(metric_string[metric_type])\n",
    "    sem_min = method_pairs_sem.min().round(4)\n",
    "    sem_max = method_pairs_sem.max().round(4)\n",
    "    plt.figtext(x=0.393, y=0.03, s=f'Standard errors: min={sem_min}, max={sem_max}', fontsize='medium')\n",
    "\n",
    "    plt.savefig(plot_path, facecolor='white', transparent=False, bbox_inches='tight', dpi=1200)\n",
    "    \n",
    "    \n",
    "def heatmap_metric_avg(method_pairs_avg, method_pairs_sem, plot_path, k_title,\n",
    "                       metric_type=['rankcorr', 'pairwise', 'overlap', 'rank', 'sign', 'ranksign'],\n",
    "                       labels=['LIME', 'KernelSHAP', 'Grad', 'Grad*Input', 'IntGrad', 'SmoothGRAD']):\n",
    "    \n",
    "    '''\n",
    "    method_pairs_avg: output of metric_all_methodpairs()\n",
    "    '''\n",
    "    \n",
    "    cmap = sns.color_palette('vlag', as_cmap=True) #diverging colormap\n",
    "    \n",
    "    #heatmap\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    sns.set(font_scale=1.5)\n",
    "    sns.heatmap(method_pairs_avg, cmap=cmap, #mask=mask, \n",
    "                vmin=-1 if metric_type=='rankcorr' else 0, vmax=1, center=0,\n",
    "                xticklabels=labels, yticklabels=labels, annot=True, fmt='.3f',\n",
    "                square=True, linewidths=.5, cbar_kws={'shrink': 0.995}) #annot_kws={'fontsize':'large'}\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    metric_string = {'rankcorr': 'Rank correlation', \n",
    "                     'pairwise': 'Pairwise rank agreement', \n",
    "                     'overlap': 'Feature agreement', \n",
    "                     'rank': 'Rank agreement', \n",
    "                     'sign': 'Sign agreement', \n",
    "                     'ranksign': 'Signed rank agreement'}\n",
    "    \n",
    "    if metric_type in ['rankcorr', 'pairwise']:\n",
    "        plt.title(metric_string[metric_type])\n",
    "    else:\n",
    "        print(metric_string,metric_type, k_title )\n",
    "        print(metric_string[metric_type])\n",
    "        plt.title(f'{metric_string[metric_type]} (k = {k_title})')\n",
    "    \n",
    "    sem_min = method_pairs_sem.min().round(3)\n",
    "    sem_max = method_pairs_sem.max().round(3)\n",
    "    plt.figtext(x=0.385, y=0.00005, s=f'Standard errors: min={sem_min}, max={sem_max}', fontsize='medium')\n",
    "\n",
    "    plt.savefig(plot_path, facecolor='white', transparent=False, bbox_inches='tight', dpi=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def heatmap_metric_sem(method_pairs_sem, plot_path, \n",
    "#                        metric_type=['rankcorr', 'pairwise', 'overlap', 'rank', 'sign', 'ranksign'],\n",
    "#                        labels=['LIME', 'KernelSHAP', 'Grad', 'Grad*Input', 'IntGrad', 'SmoothGRAD']):\n",
    "    \n",
    "#     '''\n",
    "#     method_pairs_sem: output of metric_all_methodpairs()\n",
    "#     '''\n",
    "    \n",
    "#     #mask = np.invert(np.tril(np.ones_like(corr_matrix, dtype=bool))) #mask for upper triangle\n",
    "#     cmap = sns.color_palette('vlag', as_cmap=True) #diverging colormap\n",
    "    \n",
    "#     #heatmap\n",
    "#     plt.figure(figsize=(15, 7))\n",
    "#     sns.heatmap(method_pairs_sem, cmap=cmap, #mask=mask, \n",
    "#                 #vmin=-1, vmax=1, center=0,\n",
    "#                 xticklabels=labels, yticklabels=labels, annot=True, fmt='.4f',\n",
    "#                 square=True, linewidths=.5, cbar_kws={'shrink': 0.995})\n",
    "#     plt.yticks(rotation=0)\n",
    "    \n",
    "#     metric_string = {'rankcorr': 'Rank correlation', \n",
    "#                      'pairwise': 'Pairwise comparison agreement', \n",
    "#                      'overlap': 'Overlap agreement', \n",
    "#                      'rank': 'Rank agreement', \n",
    "#                      'sign': 'Sign agreement', \n",
    "#                      'ranksign': 'Signed rank agreement'}\n",
    "    \n",
    "#     sem_min = method_pairs_sem.min().round(4)\n",
    "#     sem_max = method_pairs_sem.max().round(4)\n",
    "#     plt.title(f'{metric_string[metric_type]} \\n min={sem_min}, max={sem_max}')\n",
    "    \n",
    "#     plt.savefig(plot_path, facecolor='white', transparent=False, bbox_inches='tight', dpi=1200)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all functions\n",
    "def metric_analysis(expl_methods_names, expl_methods_attrs, k, model_name, \n",
    "                    metric_type=['rankcorr', 'pairwise', 'overlap', 'rank', 'sign', 'ranksign'],\n",
    "                    labels=['LIME', 'Kernel\\nSHAP', 'Grad', 'Grad*\\nInput', 'IntGrad', 'Smooth\\nGRAD']):\n",
    "    #calculate metric: distribution and average (for all method pairs)\n",
    "    method_pairs_distr, method_pairs_avg, method_pairs_sem = metric_all_methodpairs(expl_methods_names, expl_methods_attrs, k, metric_type)\n",
    "    \n",
    "    matplotlib.rc_file_defaults()\n",
    "    #plot boxplots of metric distributions for each method pair\n",
    "    plot_path=f'{model_name}_{metric_type}_distr.png' if metric_type in ['rankcorr', 'pairwise'] else f'{model_name}_{metric_type}_k{k}_distr.png'\n",
    "    boxplot_metric_distr(method_pairs_distr, plot_path, k_title=k, metric_type=metric_type)\n",
    "    \n",
    "    #plot heatmap of metric averages for each method pair\n",
    "    plot_path=f'{model_name}_{metric_type}_avg.png' if metric_type in ['rankcorr', 'pairwise'] else f'{model_name}_{metric_type}_k{k}_avg.png'\n",
    "    heatmap_metric_avg(method_pairs_avg, method_pairs_sem, plot_path, k_title=k, metric_type=metric_type, labels=labels)\n",
    "    \n",
    "\n",
    "    return method_pairs_distr, method_pairs_avg, method_pairs_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topk_boxplots(expl_methods_names, expl_methods_attrs, ks, plot_path):\n",
    "    #calculate values for plotting\n",
    "    metrics_dict = calc_topk_agreement_all_methods_by_k(expl_methods_names, expl_methods_attrs, ks, topk_type='avgprop')\n",
    "\n",
    "    #plot\n",
    "    fig, axes = plt.subplots(3, 5, figsize =(20, 8))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        title, metric_values = list(metrics_dict.items())[i]\n",
    "        sns.boxplot(data=metric_values, ax=ax, color='cornflowerblue')\n",
    "        ax.set_xticklabels(ks, rotation=0)\n",
    "        ax.set(xlabel='k', ylabel='Feature Agreement', title=title)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(plot_path, facecolor='white', transparent=False, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- calculating: nn, rankcorr -----------\n",
      "----------- calculating: nn, pairwise -----------\n",
      "----------- calculating: nn, overlap -----------\n",
      "k=3\n",
      "{'rankcorr': 'Rank correlation', 'pairwise': 'Pairwise rank agreement', 'overlap': 'Feature agreement', 'rank': 'Rank agreement', 'sign': 'Sign agreement', 'ranksign': 'Signed rank agreement'} overlap 3\n",
      "Feature agreement\n",
      "{'rankcorr': 'Rank correlation', 'pairwise': 'Pairwise rank agreement', 'overlap': 'Feature agreement', 'rank': 'Rank agreement', 'sign': 'Sign agreement', 'ranksign': 'Signed rank agreement'} overlap 3\n",
      "Feature agreement\n",
      "----------- calculating: nn, rank -----------\n",
      "k=3\n",
      "{'rankcorr': 'Rank correlation', 'pairwise': 'Pairwise rank agreement', 'overlap': 'Feature agreement', 'rank': 'Rank agreement', 'sign': 'Sign agreement', 'ranksign': 'Signed rank agreement'} rank 3\n",
      "Rank agreement\n",
      "{'rankcorr': 'Rank correlation', 'pairwise': 'Pairwise rank agreement', 'overlap': 'Feature agreement', 'rank': 'Rank agreement', 'sign': 'Sign agreement', 'ranksign': 'Signed rank agreement'} rank 3\n",
      "Rank agreement\n",
      "----------- calculating: nn, sign -----------\n",
      "k=3\n",
      "{'rankcorr': 'Rank correlation', 'pairwise': 'Pairwise rank agreement', 'overlap': 'Feature agreement', 'rank': 'Rank agreement', 'sign': 'Sign agreement', 'ranksign': 'Signed rank agreement'} sign 3\n",
      "Sign agreement\n",
      "{'rankcorr': 'Rank correlation', 'pairwise': 'Pairwise rank agreement', 'overlap': 'Feature agreement', 'rank': 'Rank agreement', 'sign': 'Sign agreement', 'ranksign': 'Signed rank agreement'} sign 3\n",
      "Sign agreement\n",
      "----------- calculating: nn, ranksign -----------\n",
      "k=3\n"
     ]
    }
   ],
   "source": [
    "#logistic + nn models\n",
    "expl_methods_names = {0: 'LIME',\n",
    "                1: 'KernelSHAP', \n",
    "                2: 'Grad',\n",
    "                3: 'Grad*Input',\n",
    "                4: 'IntGrad',\n",
    "                5: 'SmoothGRAD'}\n",
    "\n",
    "models_list = ['nn']\n",
    "metrics_list = ['rankcorr', 'pairwise', 'overlap', 'rank', 'sign', 'ranksign'] #['overlap', 'rank', 'sign', 'ranksign' ] #, 'pairwise', 'overlap', 'rank', 'sign', 'ranksign']\n",
    "ks_list = [3] # Change top-K here for experiments\n",
    "\n",
    "for model in models_list:\n",
    "    expl_methods_attrs = {0: limes_ex_l,\n",
    "                          1: shap_ex_l, \n",
    "                          2: gs_ex_l,\n",
    "                          3: gis_ex_l,\n",
    "                          4: igs_ex_l,\n",
    "                          5: sgs_ex_l}\n",
    "\n",
    "    for metric in metrics_list:\n",
    "        print(f'----------- calculating: {model}, {metric} -----------')\n",
    "        if metric in ['overlap', 'rank', 'sign', 'ranksign']:\n",
    "            for k in ks_list:\n",
    "                print(f'k={k}')\n",
    "                method_pairs_distr, method_pairs_avg, method_pairs_sem = metric_analysis(expl_methods_names, expl_methods_attrs, k, f'figures/{model}', metric)\n",
    "        else:\n",
    "            k=0 #arbitrary: these metrics (rankcorr and pairwise) don't use k\n",
    "            method_pairs_distr, method_pairs_avg, method_pairs_sem = metric_analysis(expl_methods_names, expl_methods_attrs, k, f'figures/{model}', metric)    \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
