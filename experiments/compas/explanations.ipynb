{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf06ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00de01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import tqdm \n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import shap as shap\n",
    "#from captum.attr import KernelShap\n",
    "from captum.attr import IntegratedGradients, Saliency, NoiseTunnel, InputXGradient\n",
    "from lime import lime_tabular\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ed7f3",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd01b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>two_year_recid</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>c_charge_degree_F</th>\n",
       "      <th>sex_Female</th>\n",
       "      <th>race</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  two_year_recid  priors_count  length_of_stay  c_charge_degree_F  \\\n",
       "0   28               1             6              80                  1   \n",
       "1   25               1             9              87                  1   \n",
       "2   33               1             5               0                  1   \n",
       "3   30               0             0               6                  1   \n",
       "4   37               1             0               1                  0   \n",
       "\n",
       "   sex_Female  race  risk  \n",
       "0           0     1     0  \n",
       "1           0     0     0  \n",
       "2           1     1     0  \n",
       "3           0     0     1  \n",
       "4           0     1     1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data, train: (3455, 8)\n",
      "data, test: (1482, 8)\n",
      "----- TRAIN -----\n",
      "X, shape: (3455, 7)\n",
      "y, shape: (3455,)\n",
      "#class1:  2805 , prop = 0.8118668596237337\n",
      "#class0: 650 , prop = 0.18813314037626627\n",
      "----- TEST -----\n",
      "X, shape: (1482, 7)\n",
      "y, shape: (1482,)\n",
      "#class1:  1217 , prop = 0.8211875843454791\n",
      "#class0: 265 , prop = 0.1788124156545209\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data_train = pd.read_csv('data/compas-scores-train-70.csv')  \n",
    "data_test = pd.read_csv('data/compas-scores-test-30.csv')  \n",
    "\n",
    "display(data_train.head())\n",
    "\n",
    "print('data, train:', data_train.shape)\n",
    "print('data, test:', data_test.shape)\n",
    "\n",
    "#split into X, y\n",
    "X_train = data_train.loc[:, data_train.columns != 'risk']\n",
    "y_train = data_train['risk']\n",
    "\n",
    "X_test = data_test.loc[:, data_test.columns != 'risk']\n",
    "y_test = data_test['risk']\n",
    "\n",
    "\n",
    "X=X_train\n",
    "y=y_train\n",
    "print('----- TRAIN -----')\n",
    "print('X, shape:', X.shape)\n",
    "print('y, shape:', y.shape)\n",
    "print('#class1: ', sum(y), f', prop = {sum(y)/len(y)}')\n",
    "print('#class0:', sum(y==0), f', prop = {sum(y==0)/len(y)}')\n",
    "\n",
    "X=X_test\n",
    "y=y_test\n",
    "print('----- TEST -----')\n",
    "print('X, shape:', X.shape)\n",
    "print('y, shape:', y.shape)\n",
    "print('#class1: ', sum(y), f', prop = {sum(y)/len(y)}')\n",
    "print('#class0:', sum(y==0), f', prop = {sum(y==0)/len(y)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae78808f",
   "metadata": {},
   "source": [
    "# load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ed435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models\n",
    "\n",
    "#####logistic regression\n",
    "model_filename = 'models/model_logistic.pkl'\n",
    "model_logistic = pickle.load(open(model_filename, 'rb'))\n",
    "\n",
    "######gradient boosted tree\n",
    "model_filename = 'models/model_gb.pkl'\n",
    "model_gb = pickle.load(open(model_filename, 'rb'))\n",
    "\n",
    "######random forest\n",
    "model_filename = 'models/model_rf.pkl'\n",
    "model_rf = pickle.load(open(model_filename, 'rb'))\n",
    "\n",
    "######FFNN\n",
    "\n",
    "#module\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, seed=12345):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        #variables\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        #layers architecture\n",
    "        self.linear_layer1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.linear_layer2 = nn.Linear(self.hidden_size, self.hidden_size*2)\n",
    "        self.linear_layer3 = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        self.linear_layer4 = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.linear_layer1(inputs)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.linear_layer2(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.linear_layer3(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.linear_layer4(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = torch.FloatTensor(X)\n",
    "        class1_probs = self.forward(X).detach().numpy()\n",
    "        class0_probs = 1-class1_probs\n",
    "        return np.hstack((class0_probs, class1_probs))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "\n",
    "model_filename = 'models/model_nn.pkl'\n",
    "model_nn = torch.load(model_filename)\n",
    "\n",
    "\n",
    "##### NN logistic regression\n",
    "\n",
    "#module\n",
    "\n",
    "#create model class\n",
    "class LogisticRegressionNN(nn.Module):\n",
    "    def __init__(self, input_size, seed=12345):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        #variables\n",
    "        self.input_size = input_size\n",
    "        #layers\n",
    "        self.linear_layer = nn.Linear(self.input_size, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.linear_layer(inputs)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = torch.tensor(X).type(torch.FloatTensor)\n",
    "        class1_probs = self.forward(X).detach().numpy()\n",
    "        class0_probs = 1-class1_probs\n",
    "        return np.hstack((class0_probs, class1_probs))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "\n",
    "model_filename = 'models/model_nn_logistic.pkl'\n",
    "model_nn_logistic = torch.load(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369b8b4",
   "metadata": {},
   "source": [
    "# ---------- METHOD 1: kernelshap ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c377bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernelshap function\n",
    "def explain_kernelshap(predict_fn, instances, filename,\n",
    "                       background_data, nsamples):\n",
    "    #run kernelshap\n",
    "    explainer = shap.KernelExplainer(model=predict_fn, data=background_data)\n",
    "    shap_values = explainer.shap_values(X=instances, nsamples=nsamples)\n",
    "    \n",
    "    #save shap values\n",
    "    pickle.dump(shap_values, open(filename, 'wb'))\n",
    "\n",
    "    return shap_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1f88693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict functions --> input for explain_kernelshap\n",
    "\n",
    "# predict_fn: predicts probability of class1\n",
    "#     in: instances, 2D np.array, n(=#datapoints) x p(=#features)\n",
    "#     out: predictions, 1D np.array, n\n",
    "\n",
    "\n",
    "def predict_fn_logistic(instances):\n",
    "    return model_logistic.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_gb(instances):\n",
    "    return model_gb.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_rf(instances):\n",
    "    return model_rf.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_nn(instances):\n",
    "    return model_nn.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_nn_logistic(instances):\n",
    "    return model_nn_logistic.predict_proba(instances)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa2d36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number instances to explain\n",
    "n= X_test.shape[0] #10\n",
    "\n",
    "#general arguments\n",
    "instances = X_test.values[0:n, :]\n",
    "\n",
    "#shap arguments\n",
    "background_data = X_test.values\n",
    "nsamples = 2**7\n",
    "\n",
    "\n",
    "model_names = ['logistic', 'gb', 'rf', 'nn', 'nn_logistic']\n",
    "predict_fns = {'logistic': predict_fn_logistic, \n",
    "               'gb': predict_fn_gb, \n",
    "               'rf': predict_fn_rf, \n",
    "               'nn': predict_fn_nn,\n",
    "               'nn_logistic': predict_fn_nn_logistic}\n",
    "filenames_ks = {'logistic': 'explanations/expl_kernelshap_logistic.pkl', \n",
    "                'gb': 'explanations/expl_kernelshap_gb.pkl', \n",
    "                'rf': 'explanations/expl_kernelshap_rf.pkl', \n",
    "                'nn': 'explanations/expl_kernelshap_nn.pkl',\n",
    "                'nn_logistic': 'explanations/expl_kernelshap_nn_logistic.pkl'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8888aa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 1482 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea31627748f342e0bb029f158ae75999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 1482 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db8efe664864fe98fa9dce80049bc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 1482 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2631c8ced7e146cdb7f4068a329684d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 1482 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2baaf31e2fbd461fa6045db8ede67158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 1482 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afde50b80f7c437180e5b126793b141f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#explain all models using kernelshap\n",
    "expl_shap = {m: explain_kernelshap(predict_fns[m], instances, filenames_ks[m], background_data, nsamples) \n",
    "             for m in model_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be3c8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load kernelshap explanations\n",
    "\n",
    "model_names = ['logistic', 'gb', 'rf', 'nn', 'nn_logistic']\n",
    "\n",
    "filenames_ks = {'logistic': 'explanations/expl_kernelshap_logistic.pkl', \n",
    "                'gb': 'explanations/expl_kernelshap_gb.pkl', \n",
    "                'rf': 'explanations/expl_kernelshap_rf.pkl', \n",
    "                'nn': 'explanations/expl_kernelshap_nn.pkl',\n",
    "                'nn_logistic': 'explanations/expl_kernelshap_nn_logistic.pkl'}\n",
    "\n",
    "\n",
    "expl_shap = {m: pickle.load(open(filenames_ks[m], 'rb')) for m in model_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df99a18",
   "metadata": {},
   "source": [
    "# ---------- METHOD 2: vanilla gradient ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4348a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_vanilla_grad(model, instances, filename):\n",
    "    model.zero_grad()\n",
    "    method = Saliency(model)\n",
    "    attr = method.attribute(instances)\n",
    "    \n",
    "    attr_np = attr.numpy()\n",
    "    pickle.dump(attr_np, open(filename, 'wb'))\n",
    "    return attr_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab7696",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e37000a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn\n",
    "filename = 'explanations/expl_vanillagrad_nn.pkl'\n",
    "\n",
    "#run explanation \n",
    "expl_vanillagrad_nn = explain_vanilla_grad(model, instances, filename)\n",
    "\n",
    "#load explanation\n",
    "expl_vanillagrad_nn = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52ebbe",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c8f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn_logistic\n",
    "filename = 'explanations/expl_vanillagrad_nn_logistic.pkl'\n",
    "\n",
    "#run explanation \n",
    "expl_vanillagrad_nn_logistic = explain_vanilla_grad(model, instances, filename)\n",
    "\n",
    "#load explanation\n",
    "expl_vanillagrad_nn_logistic = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3a99e8",
   "metadata": {},
   "source": [
    "# ---------- METHOD 3: GRADIENT*INPUT ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0912fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_gradtinput(model, instances, filename):\n",
    "    model.zero_grad()\n",
    "    method = InputXGradient(model)\n",
    "    attr = method.attribute(instances)\n",
    "    \n",
    "    attr_np = attr.detach().numpy()\n",
    "    pickle.dump(attr_np, open(filename, 'wb'))\n",
    "    return attr_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60123c1",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95cefc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn\n",
    "filename = 'explanations/expl_gradtinput_nn.pkl'\n",
    "\n",
    "#run explanation\n",
    "expl_gradtinput_nn = explain_gradtinput(model, instances, filename)\n",
    "\n",
    "#load explanation\n",
    "expl_gradtinput_nn = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae09e6",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b251f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn_logistic\n",
    "filename = 'explanations/expl_gradtinput_nn_logistic.pkl'\n",
    "\n",
    "#run explanation\n",
    "expl_gradtinput_nn_logistic = explain_gradtinput(model, instances, filename)\n",
    "\n",
    "#load explanation\n",
    "expl_gradtinput_nn_logistic = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0088bc",
   "metadata": {},
   "source": [
    "# ---------- METHOD 4: integrated gradients ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45f6d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_integrated_grad(model, instances, n_steps=50):\n",
    "    model.zero_grad()\n",
    "    method = IntegratedGradients(model)\n",
    "    attr = method.attribute(instances, n_steps=n_steps)\n",
    "    return attr.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da3307",
   "metadata": {},
   "source": [
    "## explore convergence, NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55717bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence(model, instances, nsamples_list, filename, expl_method=['integratedgrad', 'smoothgrad']):\n",
    "    #dict to store attributions for each sample_size in nsamples_list (sample_size: attributions)\n",
    "    convergence_attr = {}\n",
    "    \n",
    "    for i in nsamples_list:\n",
    "        \n",
    "        print(f'nsamples={i}')\n",
    "        start = time.time()\n",
    "        print(f'   start: {datetime.now()}')\n",
    "        \n",
    "        #run explanation method\n",
    "        if expl_method=='integratedgrad':\n",
    "            expl_i = explain_integrated_grad(model, instances, n_steps=i)\n",
    "        elif expl_method=='smoothgrad':\n",
    "            expl_i = explain_smoothgrad(model, instances, nt_samples=i)\n",
    "        #store values\n",
    "        convergence_attr[i] = expl_i\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'   stop: {datetime.now()}')\n",
    "        print(f'   duration: {(stop-start)/60} min')\n",
    "        \n",
    "    #save data\n",
    "    pickle.dump(convergence_attr, open(filename, 'wb'))\n",
    "\n",
    "    return convergence_attr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "542cc1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples=50\n",
      "   start: 2021-12-12 15:16:56.050523\n",
      "   stop: 2021-12-12 15:17:06.659217\n",
      "   duration: 0.17681156794230143 min\n",
      "nsamples=100\n",
      "   start: 2021-12-12 15:17:06.659357\n",
      "   stop: 2021-12-12 15:18:02.262260\n",
      "   duration: 0.9267150322596233 min\n",
      "nsamples=200\n",
      "   start: 2021-12-12 15:18:02.262461\n",
      "   stop: 2021-12-12 15:23:51.683379\n",
      "   duration: 5.823681934674581 min\n",
      "nsamples=400\n",
      "   start: 2021-12-12 15:23:51.683575\n",
      "   stop: 2021-12-12 15:57:02.203592\n",
      "   duration: 33.17533359924952 min\n",
      "nsamples=600\n",
      "   start: 2021-12-12 15:57:02.203774\n",
      "   stop: 2021-12-12 17:19:15.437883\n",
      "   duration: 82.22056846618652 min\n",
      "nsamples=800\n",
      "   start: 2021-12-12 17:19:15.438077\n",
      "   stop: 2021-12-12 19:48:10.578240\n",
      "   duration: 148.91900267998378 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-12 19:48:10.578435\n",
      "   stop: 2021-12-12 23:45:58.563329\n",
      "   duration: 237.7997476498286 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-12 23:45:58.564040\n",
      "   stop: 2021-12-13 08:55:44.807521\n",
      "   duration: 549.770723982652 min\n"
     ]
    }
   ],
   "source": [
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn\n",
    "\n",
    "n = X_test.shape[0] #10\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "expl_method='integratedgrad'\n",
    "filename=f'convergence/convergence_{expl_method}_nn.pkl'\n",
    "\n",
    "#check convergence\n",
    "convergence_integratedgrad_nn = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "#load file\n",
    "convergence_integratedgrad_nn = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd403ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore convergence, nn --- plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f890d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore convergence, nn --- pick and save explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac82faa",
   "metadata": {},
   "source": [
    "## explore convergence, logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "034cd9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples=50\n",
      "   start: 2021-12-13 08:55:44.842723\n",
      "   stop: 2021-12-13 08:55:51.054035\n",
      "   duration: 0.10352184772491455 min\n",
      "nsamples=100\n",
      "   start: 2021-12-13 08:55:51.054209\n",
      "   stop: 2021-12-13 08:56:39.916304\n",
      "   duration: 0.814368216196696 min\n",
      "nsamples=200\n",
      "   start: 2021-12-13 08:56:39.916427\n",
      "   stop: 2021-12-13 09:02:19.531799\n",
      "   duration: 5.660256167252858 min\n",
      "nsamples=400\n",
      "   start: 2021-12-13 09:02:19.531962\n",
      "   stop: 2021-12-13 09:34:34.884903\n",
      "   duration: 32.255882330735524 min\n",
      "nsamples=600\n",
      "   start: 2021-12-13 09:34:34.885297\n",
      "   stop: 2021-12-13 10:55:08.090098\n",
      "   duration: 80.55341333548228 min\n",
      "nsamples=800\n",
      "   start: 2021-12-13 10:55:08.090277\n",
      "   stop: 2021-12-13 13:08:25.663816\n",
      "   duration: 133.29289159377416 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-13 13:08:25.664945\n",
      "   stop: 2021-12-13 17:06:35.754324\n",
      "   duration: 238.16815556287764 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-13 17:06:35.755503\n",
      "   stop: 2021-12-14 02:05:15.161910\n",
      "   duration: 538.6567727168401 min\n"
     ]
    }
   ],
   "source": [
    "#explore convergence, logistic\n",
    "\n",
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn_logistic\n",
    "\n",
    "n = X_test.shape[0] #10\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "expl_method='integratedgrad'\n",
    "filename=f'convergence/convergence_{expl_method}_nn_logistic.pkl'\n",
    "\n",
    "\n",
    "\n",
    "#check convergence\n",
    "convergence_integratedgrad_nn_logistic = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "#load file\n",
    "convergence_integratedgrad_nn_logistic = pickle.load(open(filename, 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c167841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa22a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3a542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58cedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c10b55b",
   "metadata": {},
   "source": [
    "# ---------- METHOD 5: smoothgrad ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5553932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_smoothgrad(model, instances, nt_samples=5):\n",
    "    model.zero_grad()\n",
    "    method = NoiseTunnel(Saliency(model))\n",
    "    attr = method.attribute(instances, nt_type='smoothgrad', nt_samples=nt_samples)\n",
    "    return attr.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8540d",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d648e44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples=50\n",
      "   start: 2021-12-14 02:05:15.201352\n",
      "   stop: 2021-12-14 02:05:21.405546\n",
      "   duration: 0.10340323448181152 min\n",
      "nsamples=100\n",
      "   start: 2021-12-14 02:05:21.405654\n",
      "   stop: 2021-12-14 02:06:12.456221\n",
      "   duration: 0.8508427500724792 min\n",
      "nsamples=200\n",
      "   start: 2021-12-14 02:06:12.456395\n",
      "   stop: 2021-12-14 02:12:03.776269\n",
      "   duration: 5.855331218242645 min\n",
      "nsamples=400\n",
      "   start: 2021-12-14 02:12:03.776445\n",
      "   stop: 2021-12-14 02:45:25.377062\n",
      "   duration: 33.36001027027766 min\n",
      "nsamples=600\n",
      "   start: 2021-12-14 02:45:25.377232\n",
      "   stop: 2021-12-14 04:08:34.738138\n",
      "   duration: 83.15601507027944 min\n",
      "nsamples=800\n",
      "   start: 2021-12-14 04:08:34.738513\n",
      "   stop: 2021-12-14 06:40:11.506270\n",
      "   duration: 151.61279593308765 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-14 06:40:11.507818\n",
      "   stop: 2021-12-14 10:39:27.175391\n",
      "   duration: 239.26112620035806 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-14 10:39:27.175585\n",
      "   stop: 2021-12-14 19:34:24.491990\n",
      "   duration: 534.955272646745 min\n"
     ]
    }
   ],
   "source": [
    "#smoothgrad\n",
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn\n",
    "\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "expl_method='smoothgrad'\n",
    "filename=f'convergence/convergence_{expl_method}_nn.pkl'\n",
    "\n",
    "#check convergence\n",
    "convergence_smoothgrad_nn = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "#load file\n",
    "convergence_smoothgrad_nn = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da398bd",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb95844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples=50\n",
      "   start: 2021-12-14 19:34:24.546553\n",
      "   stop: 2021-12-14 19:34:30.729483\n",
      "   duration: 0.10304881334304809 min\n",
      "nsamples=100\n",
      "   start: 2021-12-14 19:34:30.729603\n",
      "   stop: 2021-12-14 19:35:08.190801\n",
      "   duration: 0.624353297551473 min\n",
      "nsamples=200\n",
      "   start: 2021-12-14 19:35:08.190976\n",
      "   stop: 2021-12-14 19:40:59.273244\n",
      "   duration: 5.85137110153834 min\n",
      "nsamples=400\n",
      "   start: 2021-12-14 19:40:59.273412\n",
      "   stop: 2021-12-14 20:14:07.712701\n",
      "   duration: 33.14065479834874 min\n",
      "nsamples=600\n",
      "   start: 2021-12-14 20:14:07.712903\n"
     ]
    }
   ],
   "source": [
    "#smoothgrad\n",
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn_logistic\n",
    "\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "expl_method='smoothgrad'\n",
    "filename=f'convergence/convergence_{expl_method}_nn_logistic.pkl'\n",
    "\n",
    "#check convergence\n",
    "convergence_smoothgrad_nn_logistic = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "#load file\n",
    "convergence_smoothgrad_nn_logistic = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f757f",
   "metadata": {},
   "source": [
    "# ---------- METHOD 6: lime ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cb5d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_lime(predict_fn, instances, training_data, cat_idxs, n_samples, filename,\n",
    "                 mode=['classification', 'regression'], seed=12345):\n",
    "    #create lime explainer\n",
    "    np.random.seed(seed)\n",
    "    explainer = lime_tabular.LimeTabularExplainer(training_data=training_data, \n",
    "                                                  mode=mode, \n",
    "                                                  discretize_continuous=False,\n",
    "                                                  sample_around_instance=True,\n",
    "                                                  random_state=seed, \n",
    "                                                  categorical_features=cat_idxs)\n",
    "    #explain each data point in 'instances'\n",
    "    exps = []\n",
    "    num_feat = instances.shape[1]\n",
    "    for x in instances:\n",
    "        exp = explainer.explain_instance(x, \n",
    "                                         predict_fn=predict_fn,\n",
    "                                         num_samples=n_samples,\n",
    "                                         num_features=num_feat).local_exp[1]\n",
    "        #format explanations\n",
    "        exp = sorted(exp, key=lambda tup: tup[0])\n",
    "        exp = [t[1] for t in exp]\n",
    "        exps.append(exp)\n",
    "    \n",
    "    #save explanations\n",
    "    exps = np.array(exps)\n",
    "    pickle.dump(exps, open(filename, 'wb'))\n",
    "    \n",
    "    return exps #n x p (same dimensions as 'instances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e10d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence_lime(predict_fn, instances, training_data, cat_idxs, filename_lime, mode,\n",
    "                           nsamples_list, filename_convergence):\n",
    "    #dict to store attributions for each sample_size in nsamples_list (sample_size: attributions)\n",
    "    convergence_attr = {}\n",
    "    \n",
    "    for i in nsamples_list:\n",
    "        \n",
    "        print(f'nsamples={i}')\n",
    "        start = time.time()\n",
    "        print(f'   start: {datetime.now()}')\n",
    "        \n",
    "        #run explanation method\n",
    "        expl_i = explain_lime(predict_fn, instances, training_data, cat_idxs, \n",
    "                              n_samples=i, filename=f'{filename_lime}_n{i}.pkl', mode=mode)\n",
    "        #store values\n",
    "        convergence_attr[i] = expl_i\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'   stop: {datetime.now()}')\n",
    "        print(f'   duration: {(stop-start)/60} min')\n",
    "        \n",
    "    #save data\n",
    "    pickle.dump(convergence_attr, open(filename_convergence, 'wb'))\n",
    "\n",
    "    return convergence_attr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86197943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionaries\n",
    "model_names = ['logistic', 'gb', 'rf', 'nn', 'nn_logistic']\n",
    "predict_fns_2classes = {'logistic': model_logistic.predict_proba, \n",
    "               'gb': model_gb.predict_proba, \n",
    "               'rf': model_rf.predict_proba, \n",
    "               'nn': model_nn.predict_proba,\n",
    "               'nn_logistic': model_nn_logistic.predict_proba}\n",
    "\n",
    "\n",
    "#arguments that are constant\n",
    "n = X_test.shape[0] #10\n",
    "instances = X_test.values[0:n, :]\n",
    "training_data = X_train.values\n",
    "\n",
    "cat_vars = ['two_year_recid', 'c_charge_degree_F', 'sex_Female', 'race']\n",
    "cat_idxs = [list(X_train.columns).index(var) for var in cat_vars]\n",
    "\n",
    "mode = 'classification'\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500, 2000, 2500, 3000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79e2308e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******convergence analysis for logistic******\n",
      "nsamples=50\n",
      "   start: 2021-12-14 20:59:11.698089\n",
      "   stop: 2021-12-14 20:59:16.063180\n",
      "   duration: 0.07275149822235108 min\n",
      "nsamples=100\n",
      "   start: 2021-12-14 20:59:16.063308\n",
      "   stop: 2021-12-14 20:59:20.484128\n",
      "   duration: 0.07368031342824301 min\n",
      "nsamples=200\n",
      "   start: 2021-12-14 20:59:20.484275\n",
      "   stop: 2021-12-14 20:59:25.247080\n",
      "   duration: 0.07938005526860555 min\n",
      "nsamples=400\n",
      "   start: 2021-12-14 20:59:25.247230\n",
      "   stop: 2021-12-14 20:59:29.698626\n",
      "   duration: 0.07418991724650065 min\n",
      "nsamples=600\n",
      "   start: 2021-12-14 20:59:29.698822\n",
      "   stop: 2021-12-14 20:59:34.903023\n",
      "   duration: 0.08673668305079142 min\n",
      "nsamples=800\n",
      "   start: 2021-12-14 20:59:34.903173\n",
      "   stop: 2021-12-14 20:59:40.160018\n",
      "   duration: 0.08761406342188517 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-14 20:59:40.160198\n",
      "   stop: 2021-12-14 20:59:45.553314\n",
      "   duration: 0.08988524675369262 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-14 20:59:45.553473\n",
      "   stop: 2021-12-14 20:59:51.206601\n",
      "   duration: 0.09421879847844442 min\n",
      "nsamples=2000\n",
      "   start: 2021-12-14 20:59:51.206760\n",
      "   stop: 2021-12-14 20:59:57.088234\n",
      "   duration: 0.09802455107371012 min\n",
      "nsamples=2500\n",
      "   start: 2021-12-14 20:59:57.088385\n",
      "   stop: 2021-12-14 21:00:03.324130\n",
      "   duration: 0.10392906665802001 min\n",
      "nsamples=3000\n",
      "   start: 2021-12-14 21:00:03.324314\n",
      "   stop: 2021-12-14 21:00:10.133531\n",
      "   duration: 0.11348695357640584 min\n",
      "******convergence analysis for gb******\n",
      "nsamples=50\n",
      "   start: 2021-12-14 21:00:10.135535\n",
      "   stop: 2021-12-14 21:00:16.929532\n",
      "   duration: 0.11323326826095581 min\n",
      "nsamples=100\n",
      "   start: 2021-12-14 21:00:16.929702\n",
      "   stop: 2021-12-14 21:00:23.760012\n",
      "   duration: 0.11383848190307617 min\n",
      "nsamples=200\n",
      "   start: 2021-12-14 21:00:23.760577\n",
      "   stop: 2021-12-14 21:00:31.200650\n",
      "   duration: 0.12400132815043131 min\n",
      "nsamples=400\n",
      "   start: 2021-12-14 21:00:31.200809\n",
      "   stop: 2021-12-14 21:00:39.028349\n",
      "   duration: 0.1304589867591858 min\n",
      "nsamples=600\n",
      "   start: 2021-12-14 21:00:39.028806\n",
      "   stop: 2021-12-14 21:00:47.585721\n",
      "   duration: 0.14261521498362223 min\n",
      "nsamples=800\n",
      "   start: 2021-12-14 21:00:47.585919\n",
      "   stop: 2021-12-14 21:00:56.306328\n",
      "   duration: 0.14534013271331786 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-14 21:00:56.306485\n",
      "   stop: 2021-12-14 21:01:05.620524\n",
      "   duration: 0.1552339514096578 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-14 21:01:05.620790\n",
      "   stop: 2021-12-14 21:01:16.035973\n",
      "   duration: 0.17358643611272176 min\n",
      "nsamples=2000\n",
      "   start: 2021-12-14 21:01:16.036141\n",
      "   stop: 2021-12-14 21:01:27.002253\n",
      "   duration: 0.18276851574579875 min\n",
      "nsamples=2500\n",
      "   start: 2021-12-14 21:01:27.002430\n",
      "   stop: 2021-12-14 21:01:38.757830\n",
      "   duration: 0.1959233005841573 min\n",
      "nsamples=3000\n",
      "   start: 2021-12-14 21:01:38.758043\n",
      "   stop: 2021-12-14 21:01:51.090293\n",
      "   duration: 0.20553749799728394 min\n",
      "******convergence analysis for rf******\n",
      "nsamples=50\n",
      "   start: 2021-12-14 21:01:51.092602\n",
      "   stop: 2021-12-14 21:02:04.275871\n",
      "   duration: 0.21972113053003947 min\n",
      "nsamples=100\n",
      "   start: 2021-12-14 21:02:04.275997\n",
      "   stop: 2021-12-14 21:02:17.880746\n",
      "   duration: 0.22674580017725626 min\n",
      "nsamples=200\n",
      "   start: 2021-12-14 21:02:17.880885\n",
      "   stop: 2021-12-14 21:02:32.945126\n",
      "   duration: 0.25107067028681435 min\n",
      "nsamples=400\n",
      "   start: 2021-12-14 21:02:32.945275\n",
      "   stop: 2021-12-14 21:02:48.722450\n",
      "   duration: 0.26295288403828937 min\n",
      "nsamples=600\n",
      "   start: 2021-12-14 21:02:48.722609\n",
      "   stop: 2021-12-14 21:03:06.421458\n",
      "   duration: 0.294980784257253 min\n",
      "nsamples=800\n",
      "   start: 2021-12-14 21:03:06.421611\n",
      "   stop: 2021-12-14 21:03:27.522598\n",
      "   duration: 0.3516830881436666 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-14 21:03:27.522773\n",
      "   stop: 2021-12-14 21:03:50.803653\n",
      "   duration: 0.3880146702130636 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-14 21:03:50.803814\n",
      "   stop: 2021-12-14 21:04:18.525100\n",
      "   duration: 0.4620214343070984 min\n",
      "nsamples=2000\n",
      "   start: 2021-12-14 21:04:18.525264\n",
      "   stop: 2021-12-14 21:04:48.539011\n",
      "   duration: 0.5002290805180868 min\n",
      "nsamples=2500\n",
      "   start: 2021-12-14 21:04:48.539163\n",
      "   stop: 2021-12-14 21:05:23.616688\n",
      "   duration: 0.5846253991127014 min\n",
      "nsamples=3000\n",
      "   start: 2021-12-14 21:05:23.616855\n",
      "   stop: 2021-12-14 21:06:06.097348\n",
      "   duration: 0.7080082019170125 min\n",
      "******convergence analysis for nn******\n",
      "nsamples=50\n",
      "   start: 2021-12-14 21:06:06.099865\n",
      "   stop: 2021-12-14 21:06:10.870667\n",
      "   duration: 0.07951335112253825 min\n",
      "nsamples=100\n",
      "   start: 2021-12-14 21:06:10.870835\n",
      "   stop: 2021-12-14 21:06:15.835079\n",
      "   duration: 0.08273738622665405 min\n",
      "nsamples=200\n",
      "   start: 2021-12-14 21:06:15.835228\n",
      "   stop: 2021-12-14 21:06:20.938495\n",
      "   duration: 0.08505443731943767 min\n",
      "nsamples=400\n",
      "   start: 2021-12-14 21:06:20.938631\n",
      "   stop: 2021-12-14 21:06:26.870604\n",
      "   duration: 0.09886618455251057 min\n",
      "nsamples=600\n",
      "   start: 2021-12-14 21:06:26.870754\n",
      "   stop: 2021-12-14 21:06:33.182401\n",
      "   duration: 0.10519408384958903 min\n",
      "nsamples=800\n",
      "   start: 2021-12-14 21:06:33.182537\n",
      "   stop: 2021-12-14 21:06:40.234205\n",
      "   duration: 0.1175277829170227 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-14 21:06:40.234360\n",
      "   stop: 2021-12-14 21:06:47.642254\n",
      "   duration: 0.12346488237380981 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-14 21:06:47.642412\n",
      "   stop: 2021-12-14 21:06:56.510176\n",
      "   duration: 0.14779605070749918 min\n",
      "nsamples=2000\n",
      "   start: 2021-12-14 21:06:56.510365\n",
      "   stop: 2021-12-14 21:07:06.804386\n",
      "   duration: 0.17156693538029988 min\n",
      "nsamples=2500\n",
      "   start: 2021-12-14 21:07:06.804608\n",
      "   stop: 2021-12-14 21:07:18.018780\n",
      "   duration: 0.18690286874771117 min\n",
      "nsamples=3000\n",
      "   start: 2021-12-14 21:07:18.019022\n",
      "   stop: 2021-12-14 21:07:29.812856\n",
      "   duration: 0.1965638836224874 min\n",
      "******convergence analysis for nn_logistic******\n",
      "nsamples=50\n",
      "   start: 2021-12-14 21:07:29.816016\n",
      "   stop: 2021-12-14 21:07:34.344824\n",
      "   duration: 0.07548011541366577 min\n",
      "nsamples=100\n",
      "   start: 2021-12-14 21:07:34.344988\n",
      "   stop: 2021-12-14 21:07:39.004452\n",
      "   duration: 0.07765770355860392 min\n",
      "nsamples=200\n",
      "   start: 2021-12-14 21:07:39.004630\n",
      "   stop: 2021-12-14 21:07:43.782639\n",
      "   duration: 0.0796334703763326 min\n",
      "nsamples=400\n",
      "   start: 2021-12-14 21:07:43.782773\n",
      "   stop: 2021-12-14 21:07:48.982996\n",
      "   duration: 0.0866703470547994 min\n",
      "nsamples=600\n",
      "   start: 2021-12-14 21:07:48.983170\n",
      "   stop: 2021-12-14 21:07:54.281894\n",
      "   duration: 0.088312033812205 min\n",
      "nsamples=800\n",
      "   start: 2021-12-14 21:07:54.282083\n",
      "   stop: 2021-12-14 21:07:59.967415\n",
      "   duration: 0.09475553433100382 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-14 21:07:59.967594\n",
      "   stop: 2021-12-14 21:08:06.199448\n",
      "   duration: 0.10386421680450439 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-14 21:08:06.199610\n",
      "   stop: 2021-12-14 21:08:12.999682\n",
      "   duration: 0.11333453257878622 min\n",
      "nsamples=2000\n",
      "   start: 2021-12-14 21:08:12.999862\n",
      "   stop: 2021-12-14 21:08:20.191069\n",
      "   duration: 0.11985344886779785 min\n",
      "nsamples=2500\n",
      "   start: 2021-12-14 21:08:20.191242\n",
      "   stop: 2021-12-14 21:08:27.545361\n",
      "   duration: 0.12256863514582315 min\n",
      "nsamples=3000\n",
      "   start: 2021-12-14 21:08:27.545529\n",
      "   stop: 2021-12-14 21:08:35.798206\n",
      "   duration: 0.137544580300649 min\n"
     ]
    }
   ],
   "source": [
    "#for each model\n",
    "for m in range(0, len(model_names)):\n",
    "    model = model_names[m]\n",
    "    print(f'******convergence analysis for {model}******')\n",
    "    \n",
    "    #run convergence analysis\n",
    "    filename_lime=f'explanations/expl_lime_{model}'\n",
    "    filename_convergence=f'convergence/convergence_lime_{model}.pkl'\n",
    "    _ = check_convergence_lime(predict_fn=predict_fns_2classes[model], \n",
    "                           instances=instances, \n",
    "                           training_data=training_data, \n",
    "                           cat_idxs=cat_idxs, \n",
    "                           filename_lime=filename_lime, \n",
    "                           mode=mode,\n",
    "                           nsamples_list=nsamples_list, \n",
    "                           filename_convergence=filename_convergence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41796b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e47e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af33ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
