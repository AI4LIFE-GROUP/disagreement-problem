{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf06ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00de01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import tqdm \n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import shap as shap\n",
    "#from captum.attr import KernelShap\n",
    "from captum.attr import IntegratedGradients, Saliency, NoiseTunnel, InputXGradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ed7f3",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd01b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>two_year_recid</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>c_charge_degree_F</th>\n",
       "      <th>sex_Female</th>\n",
       "      <th>race</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  two_year_recid  priors_count  length_of_stay  c_charge_degree_F  \\\n",
       "0   28               1             6              80                  1   \n",
       "1   25               1             9              87                  1   \n",
       "2   33               1             5               0                  1   \n",
       "3   30               0             0               6                  1   \n",
       "4   37               1             0               1                  0   \n",
       "\n",
       "   sex_Female  race  risk  \n",
       "0           0     1     0  \n",
       "1           0     0     0  \n",
       "2           1     1     0  \n",
       "3           0     0     1  \n",
       "4           0     1     1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data, train: (3455, 8)\n",
      "data, test: (1482, 8)\n",
      "----- TRAIN -----\n",
      "X, shape: (3455, 7)\n",
      "y, shape: (3455,)\n",
      "#class1:  2805 , prop = 0.8118668596237337\n",
      "#class0: 650 , prop = 0.18813314037626627\n",
      "----- TEST -----\n",
      "X, shape: (1482, 7)\n",
      "y, shape: (1482,)\n",
      "#class1:  1217 , prop = 0.8211875843454791\n",
      "#class0: 265 , prop = 0.1788124156545209\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data_train = pd.read_csv('data/compas-scores-train-70.csv')  \n",
    "data_test = pd.read_csv('data/compas-scores-test-30.csv')  \n",
    "\n",
    "display(data_train.head())\n",
    "\n",
    "print('data, train:', data_train.shape)\n",
    "print('data, test:', data_test.shape)\n",
    "\n",
    "#split into X, y\n",
    "X_train = data_train.loc[:, data_train.columns != 'risk']\n",
    "y_train = data_train['risk']\n",
    "\n",
    "X_test = data_test.loc[:, data_test.columns != 'risk']\n",
    "y_test = data_test['risk']\n",
    "\n",
    "\n",
    "X=X_train\n",
    "y=y_train\n",
    "print('----- TRAIN -----')\n",
    "print('X, shape:', X.shape)\n",
    "print('y, shape:', y.shape)\n",
    "print('#class1: ', sum(y), f', prop = {sum(y)/len(y)}')\n",
    "print('#class0:', sum(y==0), f', prop = {sum(y==0)/len(y)}')\n",
    "\n",
    "X=X_test\n",
    "y=y_test\n",
    "print('----- TEST -----')\n",
    "print('X, shape:', X.shape)\n",
    "print('y, shape:', y.shape)\n",
    "print('#class1: ', sum(y), f', prop = {sum(y)/len(y)}')\n",
    "print('#class0:', sum(y==0), f', prop = {sum(y==0)/len(y)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae78808f",
   "metadata": {},
   "source": [
    "# load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ed435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models\n",
    "\n",
    "#####logistic regression\n",
    "model_filename = 'models/model_logistic.pkl'\n",
    "model_logistic = pickle.load(open(model_filename, 'rb'))\n",
    "\n",
    "######gradient boosted tree\n",
    "model_filename = 'models/model_gb.pkl'\n",
    "model_gb = pickle.load(open(model_filename, 'rb'))\n",
    "\n",
    "######random forest\n",
    "model_filename = 'models/model_rf.pkl'\n",
    "model_rf = pickle.load(open(model_filename, 'rb'))\n",
    "\n",
    "######FFNN\n",
    "\n",
    "#module\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, seed=12345):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        #variables\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        #layers architecture\n",
    "        self.linear_layer1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.linear_layer2 = nn.Linear(self.hidden_size, self.hidden_size*2)\n",
    "        self.linear_layer3 = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        self.linear_layer4 = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.linear_layer1(inputs)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.linear_layer2(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.linear_layer3(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.linear_layer4(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = torch.FloatTensor(X)\n",
    "        class1_probs = self.forward(X).detach().numpy()\n",
    "        class0_probs = 1-class1_probs\n",
    "        return np.hstack((class0_probs, class1_probs))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "\n",
    "model_filename = 'models/model_nn.pkl'\n",
    "model_nn = torch.load(model_filename)\n",
    "\n",
    "\n",
    "##### NN logistic regression\n",
    "\n",
    "#module\n",
    "\n",
    "#create model class\n",
    "class LogisticRegressionNN(nn.Module):\n",
    "    def __init__(self, input_size, seed=12345):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        #variables\n",
    "        self.input_size = input_size\n",
    "        #layers\n",
    "        self.linear_layer = nn.Linear(self.input_size, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.linear_layer(inputs)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = torch.tensor(X).type(torch.FloatTensor)\n",
    "        class1_probs = self.forward(X).detach().numpy()\n",
    "        class0_probs = 1-class1_probs\n",
    "        return np.hstack((class0_probs, class1_probs))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "\n",
    "model_filename = 'models/model_nn_logistic.pkl'\n",
    "model_nn_logistic = torch.load(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369b8b4",
   "metadata": {},
   "source": [
    "# ---------- METHOD 1: kernelshap ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c377bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernelshap function\n",
    "def explain_kernelshap(predict_fn, instances, filename,\n",
    "                       background_data, nsamples):\n",
    "    #run kernelshap\n",
    "    explainer = shap.KernelExplainer(model=predict_fn, data=background_data)\n",
    "    shap_values = explainer.shap_values(X=instances, nsamples=nsamples)\n",
    "    \n",
    "    #save shap values\n",
    "    pickle.dump(shap_values, open(filename, 'wb'))\n",
    "\n",
    "    return shap_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1f88693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict functions --> input for explain_kernelshap\n",
    "\n",
    "# predict_fn: predicts probability of class1\n",
    "#     in: instances, 2D np.array, n(=#datapoints) x p(=#features)\n",
    "#     out: predictions, 1D np.array, n\n",
    "\n",
    "\n",
    "def predict_fn_logistic(instances):\n",
    "    return model_logistic.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_gb(instances):\n",
    "    return model_gb.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_rf(instances):\n",
    "    return model_rf.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_nn(instances):\n",
    "    return model_nn.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_nn_logistic(instances):\n",
    "    return model_nn_logistic.predict_proba(instances)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa2d36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number instances to explain\n",
    "n= X_test.shape[0] #10\n",
    "\n",
    "#general arguments\n",
    "predict_fn = predict_fn_nn\n",
    "instances = X_test.values[0:n, :]\n",
    "filename='explanations/kernelshap.pkl'\n",
    "\n",
    "#shap arguments\n",
    "background_data = X_test.values\n",
    "nsamples = 2**7\n",
    "\n",
    "\n",
    "model_names = ['logistic', 'gb', 'rf', 'nn', 'nn_logistic']\n",
    "predict_fns = {'logistic': predict_fn_logistic, \n",
    "               'gb': predict_fn_gb, \n",
    "               'rf': predict_fn_rf, \n",
    "               'nn': predict_fn_nn,\n",
    "               'nn_logistic': predict_fn_nn_logistic}\n",
    "filenames_ks = {'logistic': 'explanations/expl_kernelshap_logistic.pkl', \n",
    "                'gb': 'explanations/expl_kernelshap_gb.pkl', \n",
    "                'rf': 'explanations/expl_kernelshap_rf.pkl', \n",
    "                'nn': 'explanations/expl_kernelshap_nn.pkl',\n",
    "                'nn_logistic': 'explanations/expl_kernelshap_nn_logistic.pkl'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8888aa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 1482 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea31627748f342e0bb029f158ae75999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 1482 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db8efe664864fe98fa9dce80049bc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 1482 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2631c8ced7e146cdb7f4068a329684d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 1482 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2baaf31e2fbd461fa6045db8ede67158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 1482 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afde50b80f7c437180e5b126793b141f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#explain all models using kernelshap\n",
    "expl_shap = {m: explain_kernelshap(predict_fns[m], instances, filenames_ks[m], background_data, nsamples) \n",
    "             for m in model_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be3c8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load kernelshap explanations\n",
    "\n",
    "model_names = ['logistic', 'gb', 'rf', 'nn', 'nn_logistic']\n",
    "\n",
    "filenames_ks = {'logistic': 'explanations/expl_kernelshap_logistic.pkl', \n",
    "                'gb': 'explanations/expl_kernelshap_gb.pkl', \n",
    "                'rf': 'explanations/expl_kernelshap_rf.pkl', \n",
    "                'nn': 'explanations/expl_kernelshap_nn.pkl',\n",
    "                'nn_logistic': 'explanations/expl_kernelshap_nn_logistic.pkl'}\n",
    "\n",
    "\n",
    "expl_shap = {m: pickle.load(open(filenames_ks[m], 'rb')) for m in model_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb271291",
   "metadata": {},
   "source": [
    "# ---------- METHOD 2: vanilla gradient ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2536b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_vanilla_grad(model, instances):\n",
    "    model.zero_grad()\n",
    "    method = Saliency(model)\n",
    "    attr = method.attribute(instances)\n",
    "    return attr.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93024b4a",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49083fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn\n",
    "\n",
    "#run explanation \n",
    "expl_vanillagrad_nn = explain_vanilla_grad(model, instances)\n",
    "\n",
    "#save data\n",
    "filename = 'explanations/expl_vanillagrad_nn.pkl'\n",
    "pickle.dump(expl_vanillagrad_nn, open(filename, 'wb'))\n",
    "\n",
    "#load explanation\n",
    "expl_vanillagrad_nn = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9f7db",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "653cc11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn_logistic\n",
    "\n",
    "#run explanation \n",
    "expl_vanillagrad_nn_logistic = explain_vanilla_grad(model, instances)\n",
    "\n",
    "#save data\n",
    "filename = 'explanations/expl_vanillagrad_nn_logistic.pkl'\n",
    "pickle.dump(expl_vanillagrad_nn_logistic, open(filename, 'wb'))\n",
    "\n",
    "#load explanation\n",
    "expl_vanillagrad_nn_logistic = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba114361",
   "metadata": {},
   "source": [
    "# ---------- METHOD 3: GRADIENT*INPUT ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bb027c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_gradtinput(model, instances):\n",
    "    model.zero_grad()\n",
    "    method = InputXGradient(model)\n",
    "    attr = method.attribute(instances)\n",
    "    return attr.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a094d51",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "705b82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn\n",
    "\n",
    "#run explanation\n",
    "expl_gradtinput_nn = explain_gradtinput(model, instances)\n",
    "\n",
    "#save data\n",
    "filename = 'explanations/expl_gradtinput_nn.pkl'\n",
    "pickle.dump(expl_gradtinput_nn, open(filename, 'wb'))\n",
    "\n",
    "#load explanation\n",
    "expl_gradtinput_nn = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7d116",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d1ea1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn_logistic\n",
    "\n",
    "#run explanation\n",
    "expl_gradtinput_nn_logistic = explain_gradtinput(model, instances)\n",
    "\n",
    "#save data\n",
    "filename = 'explanations/expl_gradtinput_nn_logistic.pkl'\n",
    "pickle.dump(expl_gradtinput_nn_logistic, open(filename, 'wb'))\n",
    "\n",
    "#load explanation\n",
    "expl_gradtinput_nn_logistic = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0088bc",
   "metadata": {},
   "source": [
    "# ---------- METHOD 4: integrated gradients ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45f6d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_integrated_grad(model, instances, n_steps=50):\n",
    "    model.zero_grad()\n",
    "    method = IntegratedGradients(model)\n",
    "    attr = method.attribute(instances, n_steps=n_steps)\n",
    "    return attr.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da3307",
   "metadata": {},
   "source": [
    "## explore convergence, NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55717bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence(model, instances, nsamples_list, filename, expl_method=['integratedgrad', 'smoothgrad']):\n",
    "    #dict to store attributions for each sample_size in nsamples_list (sample_size: attributions)\n",
    "    convergence_attr = {}\n",
    "    \n",
    "    for i in nsamples_list:\n",
    "        \n",
    "        print(f'nsamples={i}')\n",
    "        start = time.time()\n",
    "        print(f'   start: {datetime.now()}')\n",
    "        \n",
    "        #run explanation method\n",
    "        if expl_method=='integratedgrad':\n",
    "            expl_i = explain_integrated_grad(model, instances, n_steps=i)\n",
    "        elif expl_method=='smoothgrad':\n",
    "            expl_i = explain_smoothgrad(model, instances, nt_samples=i)\n",
    "        #store values\n",
    "        convergence_attr[i] = expl_i\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'   stop: {datetime.now()}')\n",
    "        print(f'   duration: {(stop-start)/60} min')\n",
    "        \n",
    "    #save data\n",
    "    pickle.dump(convergence_attr, open(filename, 'wb'))\n",
    "\n",
    "    return convergence_attr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "542cc1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples=50\n",
      "   start: 2021-12-12 15:16:56.050523\n",
      "   stop: 2021-12-12 15:17:06.659217\n",
      "   duration: 0.17681156794230143 min\n",
      "nsamples=100\n",
      "   start: 2021-12-12 15:17:06.659357\n",
      "   stop: 2021-12-12 15:18:02.262260\n",
      "   duration: 0.9267150322596233 min\n",
      "nsamples=200\n",
      "   start: 2021-12-12 15:18:02.262461\n",
      "   stop: 2021-12-12 15:23:51.683379\n",
      "   duration: 5.823681934674581 min\n",
      "nsamples=400\n",
      "   start: 2021-12-12 15:23:51.683575\n",
      "   stop: 2021-12-12 15:57:02.203592\n",
      "   duration: 33.17533359924952 min\n",
      "nsamples=600\n",
      "   start: 2021-12-12 15:57:02.203774\n",
      "   stop: 2021-12-12 17:19:15.437883\n",
      "   duration: 82.22056846618652 min\n",
      "nsamples=800\n",
      "   start: 2021-12-12 17:19:15.438077\n",
      "   stop: 2021-12-12 19:48:10.578240\n",
      "   duration: 148.91900267998378 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-12 19:48:10.578435\n",
      "   stop: 2021-12-12 23:45:58.563329\n",
      "   duration: 237.7997476498286 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-12 23:45:58.564040\n",
      "   stop: 2021-12-13 08:55:44.807521\n",
      "   duration: 549.770723982652 min\n"
     ]
    }
   ],
   "source": [
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn\n",
    "\n",
    "n = X_test.shape[0] #10\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "expl_method='integratedgrad'\n",
    "filename=f'convergence/convergence_{expl_method}_nn.pkl'\n",
    "\n",
    "#check convergence\n",
    "convergence_integratedgrad_nn = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "#load file\n",
    "convergence_integratedgrad_nn = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd403ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore convergence, nn --- plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f890d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore convergence, nn --- pick and save explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac82faa",
   "metadata": {},
   "source": [
    "## explore convergence, logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034cd9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples=50\n",
      "   start: 2021-12-13 08:55:44.842723\n",
      "   stop: 2021-12-13 08:55:51.054035\n",
      "   duration: 0.10352184772491455 min\n",
      "nsamples=100\n",
      "   start: 2021-12-13 08:55:51.054209\n",
      "   stop: 2021-12-13 08:56:39.916304\n",
      "   duration: 0.814368216196696 min\n",
      "nsamples=200\n",
      "   start: 2021-12-13 08:56:39.916427\n",
      "   stop: 2021-12-13 09:02:19.531799\n",
      "   duration: 5.660256167252858 min\n",
      "nsamples=400\n",
      "   start: 2021-12-13 09:02:19.531962\n",
      "   stop: 2021-12-13 09:34:34.884903\n",
      "   duration: 32.255882330735524 min\n",
      "nsamples=600\n",
      "   start: 2021-12-13 09:34:34.885297\n",
      "   stop: 2021-12-13 10:55:08.090098\n",
      "   duration: 80.55341333548228 min\n",
      "nsamples=800\n",
      "   start: 2021-12-13 10:55:08.090277\n",
      "   stop: 2021-12-13 13:08:25.663816\n",
      "   duration: 133.29289159377416 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-13 13:08:25.664945\n",
      "   stop: 2021-12-13 17:06:35.754324\n",
      "   duration: 238.16815556287764 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-13 17:06:35.755503\n"
     ]
    }
   ],
   "source": [
    "#explore convergence, logistic\n",
    "\n",
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn_logistic\n",
    "\n",
    "n = X_test.shape[0] #10\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "expl_method='integratedgrad'\n",
    "filename=f'convergence/convergence_{expl_method}_nn_logistic.pkl'\n",
    "\n",
    "\n",
    "\n",
    "#check convergence\n",
    "convergence_integratedgrad_nn_logistic = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "#load file\n",
    "convergence_integratedgrad_nn_logistic = pickle.load(open(filename, 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c167841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa22a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3a542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58cedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c10b55b",
   "metadata": {},
   "source": [
    "# ---------- METHOD 5: smoothgrad ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5553932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_smoothgrad(model, instances, nt_samples=5):\n",
    "    model.zero_grad()\n",
    "    method = NoiseTunnel(Saliency(model))\n",
    "    attr = method.attribute(instances, nt_type='smoothgrad', nt_samples=nt_samples)\n",
    "    return attr.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8540d",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#smoothgrad\n",
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn\n",
    "\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "expl_method='smoothgrad'\n",
    "filename=f'convergence/convergence_{expl_method}_nn.pkl'\n",
    "\n",
    "#check convergence\n",
    "convergence_smoothgrad_nn = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "#load file\n",
    "convergence_smoothgrad_nn = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1741ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore convergence, nn --- plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed01d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore convergence, nn --- pick and save explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da398bd",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb95844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#smoothgrad\n",
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn_logistic\n",
    "\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "expl_method='smoothgrad'\n",
    "filename=f'convergence/convergence_{expl_method}_nn_logistic.pkl'\n",
    "\n",
    "#check convergence\n",
    "convergence_smoothgrad_nn_logistic = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "#load file\n",
    "convergence_smoothgrad_nn_logistic = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1fddbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f527c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff77ec65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abcfc97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c4a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2dc0dc5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8c1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5d59c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
