{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf06ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00de01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import tqdm \n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import shap as shap\n",
    "#from captum.attr import KernelShap\n",
    "from captum.attr import IntegratedGradients, Saliency, NoiseTunnel, InputXGradient\n",
    "from lime import lime_tabular\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ed7f3",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df3f407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/german-train.csv')\n",
    "test = pd.read_csv('data/german-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd01b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit-history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>amount</th>\n",
       "      <th>savings</th>\n",
       "      <th>employment-duration</th>\n",
       "      <th>installment-rate</th>\n",
       "      <th>personal-status-sex</th>\n",
       "      <th>other-debtors</th>\n",
       "      <th>...</th>\n",
       "      <th>property</th>\n",
       "      <th>age</th>\n",
       "      <th>other-installment-plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>number-credits</th>\n",
       "      <th>job</th>\n",
       "      <th>people-liable</th>\n",
       "      <th>telephone</th>\n",
       "      <th>foreign-worker</th>\n",
       "      <th>credit-risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1213</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2439</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10366</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1047</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1882</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   status  duration  credit-history  purpose  amount  savings  \\\n",
       "0       1        15               3        4    1213        4   \n",
       "1       2        24               3        4    2439        2   \n",
       "2       1        60               3        1   10366        2   \n",
       "3       4         6               1        7    1047        2   \n",
       "4       2        18               3        4    1882        2   \n",
       "\n",
       "   employment-duration  installment-rate  personal-status-sex  other-debtors  \\\n",
       "0                    5                 4                    5              1   \n",
       "1                    2                 4                    2              1   \n",
       "2                    5                 2                    5              1   \n",
       "3                    3                 2                    2              1   \n",
       "4                    3                 4                    2              1   \n",
       "\n",
       "   ...  property  age  other-installment-plans  housing  number-credits  job  \\\n",
       "0  ...         3   47                        2        3               1    3   \n",
       "1  ...         4   35                        3        3               1    3   \n",
       "2  ...         3   42                        3        3               1    4   \n",
       "3  ...         3   50                        3        3               1    2   \n",
       "4  ...         2   25                        1        2               2    3   \n",
       "\n",
       "   people-liable  telephone  foreign-worker  credit-risk  \n",
       "0              1          2               1            1  \n",
       "1              1          2               1            0  \n",
       "2              1          2               1            1  \n",
       "3              1          1               1            1  \n",
       "4              1          1               1            0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data, train: (800, 21)\n",
      "data, test: (200, 21)\n",
      "----- TRAIN -----\n",
      "X, shape: (800, 20)\n",
      "y, shape: (800,)\n",
      "#class1:  560 , prop = 0.7\n",
      "#class0: 240 , prop = 0.3\n",
      "----- TEST -----\n",
      "X, shape: (200, 20)\n",
      "y, shape: (200,)\n",
      "#class1:  140 , prop = 0.7\n",
      "#class0: 60 , prop = 0.3\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data_train = train\n",
    "data_test =test\n",
    "\n",
    "display(data_train.head())\n",
    "\n",
    "print('data, train:', data_train.shape)\n",
    "print('data, test:', data_test.shape)\n",
    "\n",
    "#split into X, y\n",
    "X_train = data_train.loc[:, data_train.columns != 'credit-risk']\n",
    "y_train = data_train['credit-risk']\n",
    "\n",
    "X_test = data_test.loc[:, data_test.columns != 'credit-risk']\n",
    "y_test = data_test['credit-risk']\n",
    "\n",
    "\n",
    "X=X_train\n",
    "y=y_train\n",
    "print('----- TRAIN -----')\n",
    "print('X, shape:', X.shape)\n",
    "print('y, shape:', y.shape)\n",
    "print('#class1: ', sum(y), f', prop = {sum(y)/len(y)}')\n",
    "print('#class0:', sum(y==0), f', prop = {sum(y==0)/len(y)}')\n",
    "\n",
    "X=X_test\n",
    "y=y_test\n",
    "print('----- TEST -----')\n",
    "print('X, shape:', X.shape)\n",
    "print('y, shape:', y.shape)\n",
    "print('#class1: ', sum(y), f', prop = {sum(y)/len(y)}')\n",
    "print('#class0:', sum(y==0), f', prop = {sum(y==0)/len(y)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae78808f",
   "metadata": {},
   "source": [
    "# load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8ed435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models\n",
    "\n",
    "#####logistic regression\n",
    "model_filename = 'models/model_logistic.pkl'\n",
    "model_logistic = pickle.load(open(model_filename, 'rb'))\n",
    "\n",
    "######gradient boosted tree\n",
    "model_filename = 'models/model_gb.pkl'\n",
    "model_gb = pickle.load(open(model_filename, 'rb'))\n",
    "\n",
    "######random forest\n",
    "model_filename = 'models/model_rf.pkl'\n",
    "model_rf = pickle.load(open(model_filename, 'rb'))\n",
    "\n",
    "######FFNN\n",
    "\n",
    "#module\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, seed=12345):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        #variables\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        #layers architecture\n",
    "        self.linear_layer1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.linear_layer2 = nn.Linear(self.hidden_size, self.hidden_size*2)\n",
    "        self.linear_layer3 = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        self.linear_layer4 = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.linear_layer1(inputs)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.linear_layer2(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.linear_layer3(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.linear_layer4(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = torch.FloatTensor(X)\n",
    "        class1_probs = self.forward(X).detach().numpy()\n",
    "        class0_probs = 1-class1_probs\n",
    "        return np.hstack((class0_probs, class1_probs))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "\n",
    "model_filename = 'models/model_nn.pkl'\n",
    "model_nn = torch.load(model_filename)\n",
    "\n",
    "\n",
    "##### NN logistic regression\n",
    "\n",
    "#module\n",
    "\n",
    "#create model class\n",
    "class LogisticRegressionNN(nn.Module):\n",
    "    def __init__(self, input_size, seed=12345):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        #variables\n",
    "        self.input_size = input_size\n",
    "        #layers\n",
    "        self.linear_layer = nn.Linear(self.input_size, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.linear_layer(inputs)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = torch.tensor(X).type(torch.FloatTensor)\n",
    "        class1_probs = self.forward(X).detach().numpy()\n",
    "        class0_probs = 1-class1_probs\n",
    "        return np.hstack((class0_probs, class1_probs))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "\n",
    "model_filename = 'models/model_nn_logistic.pkl'\n",
    "model_nn_logistic = torch.load(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369b8b4",
   "metadata": {},
   "source": [
    "# ---------- METHOD 1: kernelshap ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c377bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernelshap function\n",
    "def explain_kernelshap(predict_fn, instances, filename,\n",
    "                       background_data, nsamples):\n",
    "    #run kernelshap\n",
    "    explainer = shap.KernelExplainer(model=predict_fn, data=background_data)\n",
    "    shap_values = explainer.shap_values(X=instances, nsamples=nsamples)\n",
    "    \n",
    "    #save shap values\n",
    "    pickle.dump(shap_values, open(filename, 'wb'))\n",
    "\n",
    "    return shap_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1f88693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict functions --> input for explain_kernelshap\n",
    "\n",
    "# predict_fn: predicts probability of class1\n",
    "#     in: instances, 2D np.array, n(=#datapoints) x p(=#features)\n",
    "#     out: predictions, 1D np.array, n\n",
    "\n",
    "\n",
    "def predict_fn_logistic(instances):\n",
    "    return model_logistic.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_gb(instances):\n",
    "    return model_gb.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_rf(instances):\n",
    "    return model_rf.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_nn(instances):\n",
    "    return model_nn.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_nn_logistic(instances):\n",
    "    return model_nn_logistic.predict_proba(instances)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa2d36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number instances to explain\n",
    "n= X_test.shape[0] #10\n",
    "\n",
    "#general arguments\n",
    "instances = X_test.values[0:n, :]\n",
    "\n",
    "#shap arguments\n",
    "background_data = X_test.values\n",
    "nsamples = 2**7\n",
    "\n",
    "\n",
    "model_names = ['logistic', 'gb', 'rf', 'nn', 'nn_logistic']\n",
    "predict_fns = {'logistic': predict_fn_logistic, \n",
    "               'gb': predict_fn_gb, \n",
    "               'rf': predict_fn_rf, \n",
    "               'nn': predict_fn_nn,\n",
    "               'nn_logistic': predict_fn_nn_logistic}\n",
    "filenames_ks = {'logistic': 'explanations/expl_kernelshap_logistic.pkl', \n",
    "                'gb': 'explanations/expl_kernelshap_gb.pkl', \n",
    "                'rf': 'explanations/expl_kernelshap_rf.pkl', \n",
    "                'nn': 'explanations/expl_kernelshap_nn.pkl',\n",
    "                'nn_logistic': 'explanations/expl_kernelshap_nn_logistic.pkl'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8888aa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71d36e55c6a4b8299bf6e042f07ea33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e820bd310cee4eebb2ab13247e5880b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f336fccdc8438b91f4abde18523838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3775ed910146ab851e0ebcd5214e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2047bae099d94e73933614b2b6484562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#explain all models using kernelshap\n",
    "expl_shap = {m: explain_kernelshap(predict_fns[m], instances, filenames_ks[m], background_data, nsamples) \n",
    "             for m in model_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load kernelshap explanations\n",
    "\n",
    "model_names = ['logistic', 'gb', 'rf', 'nn', 'nn_logistic']\n",
    "\n",
    "filenames_ks = {'logistic': 'explanations/expl_kernelshap_logistic.pkl', \n",
    "                'gb': 'explanations/expl_kernelshap_gb.pkl', \n",
    "                'rf': 'explanations/expl_kernelshap_rf.pkl', \n",
    "                'nn': 'explanations/expl_kernelshap_nn.pkl',\n",
    "                'nn_logistic': 'explanations/expl_kernelshap_nn_logistic.pkl'}\n",
    "\n",
    "\n",
    "expl_shap = {m: pickle.load(open(filenames_ks[m], 'rb')) for m in model_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df99a18",
   "metadata": {},
   "source": [
    "# ---------- METHOD 2: vanilla gradient ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4348a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_vanilla_grad(model, instances, filename):\n",
    "    model.zero_grad()\n",
    "    method = Saliency(model)\n",
    "    attr = method.attribute(instances)\n",
    "    \n",
    "    attr_np = attr.numpy()\n",
    "    pickle.dump(attr_np, open(filename, 'wb'))\n",
    "    return attr_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab7696",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e37000a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn\n",
    "filename = 'explanations/expl_vanillagrad_nn.pkl'\n",
    "\n",
    "#run explanation \n",
    "expl_vanillagrad_nn = explain_vanilla_grad(model, instances, filename)\n",
    "\n",
    "#load explanation\n",
    "expl_vanillagrad_nn = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52ebbe",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4c8f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn_logistic\n",
    "filename = 'explanations/expl_vanillagrad_nn_logistic.pkl'\n",
    "\n",
    "#run explanation \n",
    "expl_vanillagrad_nn_logistic = explain_vanilla_grad(model, instances, filename)\n",
    "\n",
    "#load explanation\n",
    "expl_vanillagrad_nn_logistic = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3a99e8",
   "metadata": {},
   "source": [
    "# ---------- METHOD 3: GRADIENT*INPUT ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0912fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_gradtinput(model, instances, filename):\n",
    "    model.zero_grad()\n",
    "    method = InputXGradient(model)\n",
    "    attr = method.attribute(instances)\n",
    "    \n",
    "    attr_np = attr.detach().numpy()\n",
    "    pickle.dump(attr_np, open(filename, 'wb'))\n",
    "    return attr_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60123c1",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95cefc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn\n",
    "filename = 'explanations/expl_gradtinput_nn.pkl'\n",
    "\n",
    "#run explanation\n",
    "expl_gradtinput_nn = explain_gradtinput(model, instances, filename)\n",
    "\n",
    "#load explanation\n",
    "expl_gradtinput_nn = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae09e6",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b251f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn_logistic\n",
    "filename = 'explanations/expl_gradtinput_nn_logistic.pkl'\n",
    "\n",
    "#run explanation\n",
    "expl_gradtinput_nn_logistic = explain_gradtinput(model, instances, filename)\n",
    "\n",
    "#load explanation\n",
    "expl_gradtinput_nn_logistic = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0088bc",
   "metadata": {},
   "source": [
    "# ---------- METHOD 4: integrated gradients ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45f6d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_integrated_grad(model, instances, n_steps=50):\n",
    "    model.zero_grad()\n",
    "    method = IntegratedGradients(model)\n",
    "    attr = method.attribute(instances, n_steps=n_steps)\n",
    "    return attr.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da3307",
   "metadata": {},
   "source": [
    "## explore convergence, NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55717bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence(model, instances, nsamples_list, filename, expl_method=['integratedgrad', 'smoothgrad']):\n",
    "    #dict to store attributions for each sample_size in nsamples_list (sample_size: attributions)\n",
    "    convergence_attr = {}\n",
    "    \n",
    "    for i in nsamples_list:\n",
    "        \n",
    "        print(f'nsamples={i}')\n",
    "        start = time.time()\n",
    "        print(f'   start: {datetime.now()}')\n",
    "        \n",
    "        #run explanation method\n",
    "        if expl_method=='integratedgrad':\n",
    "            expl_i = explain_integrated_grad(model, instances, n_steps=i)\n",
    "        elif expl_method=='smoothgrad':\n",
    "            expl_i = explain_smoothgrad(model, instances, nt_samples=i)\n",
    "        #store values\n",
    "        convergence_attr[i] = expl_i\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'   stop: {datetime.now()}')\n",
    "        print(f'   duration: {(stop-start)/60} min')\n",
    "        \n",
    "    #save data\n",
    "    pickle.dump(convergence_attr, open(filename, 'wb'))\n",
    "\n",
    "    return convergence_attr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "542cc1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples=50\n",
      "   start: 2021-12-14 22:28:09.066224\n",
      "   stop: 2021-12-14 22:28:09.244316\n",
      "   duration: 0.002968287467956543 min\n",
      "nsamples=100\n",
      "   start: 2021-12-14 22:28:09.244603\n",
      "   stop: 2021-12-14 22:28:09.690815\n",
      "   duration: 0.007436811923980713 min\n",
      "nsamples=200\n",
      "   start: 2021-12-14 22:28:09.690945\n",
      "   stop: 2021-12-14 22:28:11.162305\n",
      "   duration: 0.024522634347279866 min\n",
      "nsamples=400\n",
      "   start: 2021-12-14 22:28:11.162429\n",
      "   stop: 2021-12-14 22:28:18.162519\n",
      "   duration: 0.11666810115178426 min\n",
      "nsamples=600\n",
      "   start: 2021-12-14 22:28:18.162664\n",
      "   stop: 2021-12-14 22:28:39.869158\n",
      "   duration: 0.3617744525273641 min\n",
      "nsamples=800\n",
      "   start: 2021-12-14 22:28:39.869667\n",
      "   stop: 2021-12-14 22:29:24.842118\n",
      "   duration: 0.7495404322942097 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-14 22:29:24.842697\n",
      "   stop: 2021-12-14 22:30:39.649159\n",
      "   duration: 1.246773934364319 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-14 22:30:39.649552\n",
      "   stop: 2021-12-14 22:33:21.984269\n",
      "   duration: 2.705578104654948 min\n"
     ]
    }
   ],
   "source": [
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn\n",
    "\n",
    "n = X_test.shape[0] #10\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "expl_method='integratedgrad'\n",
    "filename=f'convergence/convergence_{expl_method}_nn.pkl'\n",
    "\n",
    "#check convergence\n",
    "convergence_integratedgrad_nn = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "#load file\n",
    "convergence_integratedgrad_nn = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd403ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore convergence, nn --- plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f890d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore convergence, nn --- pick and save explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac82faa",
   "metadata": {},
   "source": [
    "## explore convergence, logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "034cd9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples=50\n",
      "   start: 2021-12-14 22:33:22.011389\n",
      "   stop: 2021-12-14 22:33:22.134502\n",
      "   duration: 0.002051834265391032 min\n",
      "nsamples=100\n",
      "   start: 2021-12-14 22:33:22.134664\n",
      "   stop: 2021-12-14 22:33:22.529890\n",
      "   duration: 0.0065870523452758786 min\n",
      "nsamples=200\n",
      "   start: 2021-12-14 22:33:22.530025\n",
      "   stop: 2021-12-14 22:33:24.018411\n",
      "   duration: 0.02480638027191162 min\n",
      "nsamples=400\n",
      "   start: 2021-12-14 22:33:24.018562\n",
      "   stop: 2021-12-14 22:33:30.820982\n",
      "   duration: 0.113373597462972 min\n",
      "nsamples=600\n",
      "   start: 2021-12-14 22:33:30.821115\n",
      "   stop: 2021-12-14 22:33:51.497825\n",
      "   duration: 0.3446115175882975 min\n",
      "nsamples=800\n",
      "   start: 2021-12-14 22:33:51.498141\n",
      "   stop: 2021-12-14 22:34:34.977672\n",
      "   duration: 0.7246584057807922 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-14 22:34:34.977992\n",
      "   stop: 2021-12-14 22:35:44.845328\n",
      "   duration: 1.1644553542137146 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-14 22:35:44.845652\n",
      "   stop: 2021-12-14 22:38:20.423550\n",
      "   duration: 2.592964434623718 min\n"
     ]
    }
   ],
   "source": [
    "#explore convergence, logistic\n",
    "\n",
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn_logistic\n",
    "\n",
    "n = X_test.shape[0] #10\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "expl_method='integratedgrad'\n",
    "filename=f'convergence/convergence_{expl_method}_nn_logistic.pkl'\n",
    "\n",
    "\n",
    "\n",
    "#check convergence\n",
    "convergence_integratedgrad_nn_logistic = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "#load file\n",
    "convergence_integratedgrad_nn_logistic = pickle.load(open(filename, 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c167841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa22a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3a542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58cedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c10b55b",
   "metadata": {},
   "source": [
    "# ---------- METHOD 5: smoothgrad ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5553932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_smoothgrad(model, instances, nt_samples=5):\n",
    "    model.zero_grad()\n",
    "    method = NoiseTunnel(Saliency(model))\n",
    "    attr = method.attribute(instances, nt_type='smoothgrad', nt_samples=nt_samples)\n",
    "    return attr.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8540d",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d648e44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples=50\n",
      "   start: 2021-12-14 22:38:20.443763\n",
      "   stop: 2021-12-14 22:38:20.640198\n",
      "   duration: 0.0032739837964375815 min\n",
      "nsamples=100\n",
      "   start: 2021-12-14 22:38:20.640278\n",
      "   stop: 2021-12-14 22:38:21.080281\n",
      "   duration: 0.00733335018157959 min\n",
      "nsamples=200\n",
      "   start: 2021-12-14 22:38:21.080781\n",
      "   stop: 2021-12-14 22:38:22.596869\n",
      "   duration: 0.02526810169219971 min\n",
      "nsamples=400\n",
      "   start: 2021-12-14 22:38:22.597148\n",
      "   stop: 2021-12-14 22:38:27.896745\n",
      "   duration: 0.08832656542460124 min\n",
      "nsamples=600\n",
      "   start: 2021-12-14 22:38:27.896875\n",
      "   stop: 2021-12-14 22:38:46.835533\n",
      "   duration: 0.3156442681948344 min\n",
      "nsamples=800\n",
      "   start: 2021-12-14 22:38:46.835653\n",
      "   stop: 2021-12-14 22:39:28.971945\n",
      "   duration: 0.702271330356598 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-14 22:39:28.972205\n",
      "   stop: 2021-12-14 22:40:45.289795\n",
      "   duration: 1.2719593445460002 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-14 22:40:45.290240\n",
      "   stop: 2021-12-14 22:43:34.562798\n",
      "   duration: 2.8212087313334147 min\n"
     ]
    }
   ],
   "source": [
    "#smoothgrad\n",
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn\n",
    "\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "expl_method='smoothgrad'\n",
    "filename=f'convergence/convergence_{expl_method}_nn.pkl'\n",
    "\n",
    "#check convergence\n",
    "convergence_smoothgrad_nn = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "#load file\n",
    "convergence_smoothgrad_nn = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da398bd",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "feb95844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples=50\n",
      "   start: 2021-12-14 22:43:34.625307\n",
      "   stop: 2021-12-14 22:43:34.745562\n",
      "   duration: 0.002004249890645345 min\n",
      "nsamples=100\n",
      "   start: 2021-12-14 22:43:34.745690\n",
      "   stop: 2021-12-14 22:43:35.133826\n",
      "   duration: 0.00646888017654419 min\n",
      "nsamples=200\n",
      "   start: 2021-12-14 22:43:35.137050\n",
      "   stop: 2021-12-14 22:43:36.539881\n",
      "   duration: 0.023380466302235923 min\n",
      "nsamples=400\n",
      "   start: 2021-12-14 22:43:36.540301\n",
      "   stop: 2021-12-14 22:43:43.261272\n",
      "   duration: 0.1120161493619283 min\n",
      "nsamples=600\n",
      "   start: 2021-12-14 22:43:43.261397\n",
      "   stop: 2021-12-14 22:44:04.459182\n",
      "   duration: 0.3532963832219442 min\n",
      "nsamples=800\n",
      "   start: 2021-12-14 22:44:04.459325\n",
      "   stop: 2021-12-14 22:44:50.362570\n",
      "   duration: 0.7650536696116129 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-14 22:44:50.363019\n",
      "   stop: 2021-12-14 22:46:08.352017\n",
      "   duration: 1.299816115697225 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-14 22:46:08.352683\n",
      "   stop: 2021-12-14 22:48:54.670974\n",
      "   duration: 2.7719712018966676 min\n"
     ]
    }
   ],
   "source": [
    "#smoothgrad\n",
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn_logistic\n",
    "\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "expl_method='smoothgrad'\n",
    "filename=f'convergence/convergence_{expl_method}_nn_logistic.pkl'\n",
    "\n",
    "#check convergence\n",
    "convergence_smoothgrad_nn_logistic = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "#load file\n",
    "convergence_smoothgrad_nn_logistic = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f757f",
   "metadata": {},
   "source": [
    "# ---------- METHOD 6: lime ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cb5d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_lime(predict_fn, instances, training_data, cat_idxs, n_samples, filename,\n",
    "                 mode=['classification', 'regression'], seed=12345):\n",
    "    #create lime explainer\n",
    "    np.random.seed(seed)\n",
    "    explainer = lime_tabular.LimeTabularExplainer(training_data=training_data, \n",
    "                                                  mode=mode, \n",
    "                                                  discretize_continuous=False,\n",
    "                                                  sample_around_instance=True,\n",
    "                                                  random_state=seed, \n",
    "                                                  categorical_features=cat_idxs)\n",
    "    #explain each data point in 'instances'\n",
    "    exps = []\n",
    "    num_feat = instances.shape[1]\n",
    "    for x in instances:\n",
    "        exp = explainer.explain_instance(x, \n",
    "                                         predict_fn=predict_fn,\n",
    "                                         num_samples=n_samples,\n",
    "                                         num_features=num_feat).local_exp[1]\n",
    "        #format explanations\n",
    "        exp = sorted(exp, key=lambda tup: tup[0])\n",
    "        exp = [t[1] for t in exp]\n",
    "        exps.append(exp)\n",
    "    \n",
    "    #save explanations\n",
    "    exps = np.array(exps)\n",
    "    pickle.dump(exps, open(filename, 'wb'))\n",
    "    \n",
    "    return exps #n x p (same dimensions as 'instances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e10d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence_lime(predict_fn, instances, training_data, cat_idxs, filename_lime, mode,\n",
    "                           nsamples_list, filename_convergence):\n",
    "    #dict to store attributions for each sample_size in nsamples_list (sample_size: attributions)\n",
    "    convergence_attr = {}\n",
    "    \n",
    "    for i in nsamples_list:\n",
    "        \n",
    "        print(f'nsamples={i}')\n",
    "        start = time.time()\n",
    "        print(f'   start: {datetime.now()}')\n",
    "        \n",
    "        #run explanation method\n",
    "        expl_i = explain_lime(predict_fn, instances, training_data, cat_idxs, \n",
    "                              n_samples=i, filename=f'{filename_lime}_n{i}.pkl', mode=mode)\n",
    "        #store values\n",
    "        convergence_attr[i] = expl_i\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'   stop: {datetime.now()}')\n",
    "        print(f'   duration: {(stop-start)/60} min')\n",
    "        \n",
    "    #save data\n",
    "    pickle.dump(convergence_attr, open(filename_convergence, 'wb'))\n",
    "\n",
    "    return convergence_attr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86197943",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'two_year_recid' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-05503bb912a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcat_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'two_year_recid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c_charge_degree_F'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sex_Female'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'race'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mcat_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'classification'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-05503bb912a1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcat_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'two_year_recid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c_charge_degree_F'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sex_Female'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'race'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mcat_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'classification'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'two_year_recid' is not in list"
     ]
    }
   ],
   "source": [
    "#dictionaries\n",
    "model_names = ['logistic', 'gb', 'rf', 'nn', 'nn_logistic']\n",
    "predict_fns_2classes = {'logistic': model_logistic.predict_proba, \n",
    "               'gb': model_gb.predict_proba, \n",
    "               'rf': model_rf.predict_proba, \n",
    "               'nn': model_nn.predict_proba,\n",
    "               'nn_logistic': model_nn_logistic.predict_proba}\n",
    "\n",
    "\n",
    "#arguments that are constant\n",
    "n = X_test.shape[0] #10\n",
    "instances = X_test.values[0:n, :]\n",
    "training_data = X_train.values\n",
    "\n",
    "cat_vars = ['two_year_recid', 'c_charge_degree_F', 'sex_Female', 'race']\n",
    "cat_idxs = [list(X_train.columns).index(var) for var in cat_vars]\n",
    "\n",
    "mode = 'classification'\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500, 2000, 2500, 3000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2308e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for each model\n",
    "for m in range(0, len(model_names)):\n",
    "    model = model_names[m]\n",
    "    print(f'******convergence analysis for {model}******')\n",
    "    \n",
    "    #run convergence analysis\n",
    "    filename_lime=f'explanations/expl_lime_{model}'\n",
    "    filename_convergence=f'convergence/convergence_lime_{model}.pkl'\n",
    "    _ = check_convergence_lime(predict_fn=predict_fns_2classes[model], \n",
    "                           instances=instances, \n",
    "                           training_data=training_data, \n",
    "                           cat_idxs=cat_idxs, \n",
    "                           filename_lime=filename_lime, \n",
    "                           mode=mode,\n",
    "                           nsamples_list=nsamples_list, \n",
    "                           filename_convergence=filename_convergence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41796b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e47e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af33ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
