{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf06ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00de01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import tqdm \n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import shap as shap\n",
    "#from captum.attr import KernelShap\n",
    "from captum.attr import IntegratedGradients, Saliency, NoiseTunnel, InputXGradient\n",
    "from lime import lime_tabular\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ed7f3",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df3f407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/german-train.csv')\n",
    "test = pd.read_csv('data/german-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd01b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit-history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>amount</th>\n",
       "      <th>savings</th>\n",
       "      <th>employment-duration</th>\n",
       "      <th>installment-rate</th>\n",
       "      <th>personal-status-sex</th>\n",
       "      <th>other-debtors</th>\n",
       "      <th>...</th>\n",
       "      <th>property</th>\n",
       "      <th>age</th>\n",
       "      <th>other-installment-plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>number-credits</th>\n",
       "      <th>job</th>\n",
       "      <th>people-liable</th>\n",
       "      <th>telephone</th>\n",
       "      <th>foreign-worker</th>\n",
       "      <th>credit-risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1213</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2439</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10366</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1047</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1882</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   status  duration  credit-history  purpose  amount  savings  \\\n",
       "0       1        15               3        4    1213        4   \n",
       "1       2        24               3        4    2439        2   \n",
       "2       1        60               3        1   10366        2   \n",
       "3       4         6               1        7    1047        2   \n",
       "4       2        18               3        4    1882        2   \n",
       "\n",
       "   employment-duration  installment-rate  personal-status-sex  other-debtors  \\\n",
       "0                    5                 4                    5              1   \n",
       "1                    2                 4                    2              1   \n",
       "2                    5                 2                    5              1   \n",
       "3                    3                 2                    2              1   \n",
       "4                    3                 4                    2              1   \n",
       "\n",
       "   ...  property  age  other-installment-plans  housing  number-credits  job  \\\n",
       "0  ...         3   47                        2        3               1    3   \n",
       "1  ...         4   35                        3        3               1    3   \n",
       "2  ...         3   42                        3        3               1    4   \n",
       "3  ...         3   50                        3        3               1    2   \n",
       "4  ...         2   25                        1        2               2    3   \n",
       "\n",
       "   people-liable  telephone  foreign-worker  credit-risk  \n",
       "0              1          2               1            1  \n",
       "1              1          2               1            0  \n",
       "2              1          2               1            1  \n",
       "3              1          1               1            1  \n",
       "4              1          1               1            0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data, train: (800, 21)\n",
      "data, test: (200, 21)\n",
      "----- TRAIN -----\n",
      "X, shape: (800, 20)\n",
      "y, shape: (800,)\n",
      "#class1:  560 , prop = 0.7\n",
      "#class0: 240 , prop = 0.3\n",
      "----- TEST -----\n",
      "X, shape: (200, 20)\n",
      "y, shape: (200,)\n",
      "#class1:  140 , prop = 0.7\n",
      "#class0: 60 , prop = 0.3\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data_train = train\n",
    "data_test =test\n",
    "\n",
    "display(data_train.head())\n",
    "\n",
    "print('data, train:', data_train.shape)\n",
    "print('data, test:', data_test.shape)\n",
    "\n",
    "#split into X, y\n",
    "X_train = data_train.loc[:, data_train.columns != 'credit-risk']\n",
    "y_train = data_train['credit-risk']\n",
    "\n",
    "X_test = data_test.loc[:, data_test.columns != 'credit-risk']\n",
    "y_test = data_test['credit-risk']\n",
    "\n",
    "\n",
    "X=X_train\n",
    "y=y_train\n",
    "print('----- TRAIN -----')\n",
    "print('X, shape:', X.shape)\n",
    "print('y, shape:', y.shape)\n",
    "print('#class1: ', sum(y), f', prop = {sum(y)/len(y)}')\n",
    "print('#class0:', sum(y==0), f', prop = {sum(y==0)/len(y)}')\n",
    "\n",
    "X=X_test\n",
    "y=y_test\n",
    "print('----- TEST -----')\n",
    "print('X, shape:', X.shape)\n",
    "print('y, shape:', y.shape)\n",
    "print('#class1: ', sum(y), f', prop = {sum(y)/len(y)}')\n",
    "print('#class0:', sum(y==0), f', prop = {sum(y==0)/len(y)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae78808f",
   "metadata": {},
   "source": [
    "# load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8ed435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models\n",
    "\n",
    "#####logistic regression\n",
    "model_filename = 'models/model_logistic.pkl'\n",
    "model_logistic = pickle.load(open(model_filename, 'rb'))\n",
    "\n",
    "######gradient boosted tree\n",
    "model_filename = 'models/model_gb.pkl'\n",
    "model_gb = pickle.load(open(model_filename, 'rb'))\n",
    "\n",
    "######random forest\n",
    "model_filename = 'models/model_rf.pkl'\n",
    "model_rf = pickle.load(open(model_filename, 'rb'))\n",
    "\n",
    "######FFNN\n",
    "\n",
    "#module\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, seed=12345):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        #variables\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        #layers architecture\n",
    "        self.linear_layer1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.linear_layer2 = nn.Linear(self.hidden_size, self.hidden_size*2)\n",
    "        self.linear_layer3 = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        self.linear_layer4 = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.linear_layer1(inputs)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.linear_layer2(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.linear_layer3(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.linear_layer4(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = torch.FloatTensor(X)\n",
    "        class1_probs = self.forward(X).detach().numpy()\n",
    "        class0_probs = 1-class1_probs\n",
    "        return np.hstack((class0_probs, class1_probs))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "\n",
    "model_filename = 'models/model_nn.pkl'\n",
    "model_nn = torch.load(model_filename)\n",
    "\n",
    "\n",
    "##### NN logistic regression\n",
    "\n",
    "#module\n",
    "\n",
    "#create model class\n",
    "class LogisticRegressionNN(nn.Module):\n",
    "    def __init__(self, input_size, seed=12345):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        #variables\n",
    "        self.input_size = input_size\n",
    "        #layers\n",
    "        self.linear_layer = nn.Linear(self.input_size, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.linear_layer(inputs)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = torch.tensor(X).type(torch.FloatTensor)\n",
    "        class1_probs = self.forward(X).detach().numpy()\n",
    "        class0_probs = 1-class1_probs\n",
    "        return np.hstack((class0_probs, class1_probs))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "\n",
    "model_filename = 'models/model_nn_logistic.pkl'\n",
    "model_nn_logistic = torch.load(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369b8b4",
   "metadata": {},
   "source": [
    "# ---------- METHOD 1: kernelshap ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c377bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernelshap function\n",
    "def explain_kernelshap(predict_fn, instances, filename,\n",
    "                       background_data, nsamples):\n",
    "    #run kernelshap\n",
    "    explainer = shap.KernelExplainer(model=predict_fn, data=background_data)\n",
    "    shap_values = explainer.shap_values(X=instances, nsamples=nsamples)\n",
    "    \n",
    "    #save shap values\n",
    "    pickle.dump(shap_values, open(filename, 'wb'))\n",
    "\n",
    "    return shap_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1f88693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict functions --> input for explain_kernelshap\n",
    "\n",
    "# predict_fn: predicts probability of class1\n",
    "#     in: instances, 2D np.array, n(=#datapoints) x p(=#features)\n",
    "#     out: predictions, 1D np.array, n\n",
    "\n",
    "\n",
    "def predict_fn_logistic(instances):\n",
    "    return model_logistic.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_gb(instances):\n",
    "    return model_gb.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_rf(instances):\n",
    "    return model_rf.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_nn(instances):\n",
    "    return model_nn.predict_proba(instances)[:, 1]\n",
    "\n",
    "def predict_fn_nn_logistic(instances):\n",
    "    return model_nn_logistic.predict_proba(instances)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa2d36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number instances to explain\n",
    "n= X_test.shape[0] #10\n",
    "\n",
    "#general arguments\n",
    "instances = X_test.values[0:n, :]\n",
    "\n",
    "#shap arguments\n",
    "background_data = X_test.values\n",
    "nsamples = 2**7\n",
    "\n",
    "\n",
    "model_names = ['logistic', 'gb', 'rf', 'nn', 'nn_logistic']\n",
    "predict_fns = {'logistic': predict_fn_logistic, \n",
    "               'gb': predict_fn_gb, \n",
    "               'rf': predict_fn_rf, \n",
    "               'nn': predict_fn_nn,\n",
    "               'nn_logistic': predict_fn_nn_logistic}\n",
    "filenames_ks = {'logistic': 'explanations/expl_kernelshap_logistic.pkl', \n",
    "                'gb': 'explanations/expl_kernelshap_gb.pkl', \n",
    "                'rf': 'explanations/expl_kernelshap_rf.pkl', \n",
    "                'nn': 'explanations/expl_kernelshap_nn.pkl',\n",
    "                'nn_logistic': 'explanations/expl_kernelshap_nn_logistic.pkl'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8888aa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71d36e55c6a4b8299bf6e042f07ea33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e820bd310cee4eebb2ab13247e5880b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f336fccdc8438b91f4abde18523838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3775ed910146ab851e0ebcd5214e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2047bae099d94e73933614b2b6484562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#explain all models using kernelshap\n",
    "expl_shap = {m: explain_kernelshap(predict_fns[m], instances, filenames_ks[m], background_data, nsamples) \n",
    "             for m in model_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load kernelshap explanations\n",
    "\n",
    "model_names = ['logistic', 'gb', 'rf', 'nn', 'nn_logistic']\n",
    "\n",
    "filenames_ks = {'logistic': 'explanations/expl_kernelshap_logistic.pkl', \n",
    "                'gb': 'explanations/expl_kernelshap_gb.pkl', \n",
    "                'rf': 'explanations/expl_kernelshap_rf.pkl', \n",
    "                'nn': 'explanations/expl_kernelshap_nn.pkl',\n",
    "                'nn_logistic': 'explanations/expl_kernelshap_nn_logistic.pkl'}\n",
    "\n",
    "\n",
    "expl_shap = {m: pickle.load(open(filenames_ks[m], 'rb')) for m in model_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df99a18",
   "metadata": {},
   "source": [
    "# ---------- METHOD 2: vanilla gradient ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4348a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_vanilla_grad(model, instances, filename):\n",
    "    model.zero_grad()\n",
    "    method = Saliency(model)\n",
    "    attr = method.attribute(instances)\n",
    "    \n",
    "    attr_np = attr.numpy()\n",
    "    pickle.dump(attr_np, open(filename, 'wb'))\n",
    "    return attr_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab7696",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e37000a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn\n",
    "filename = 'explanations/expl_vanillagrad_nn.pkl'\n",
    "\n",
    "#run explanation \n",
    "expl_vanillagrad_nn = explain_vanilla_grad(model, instances, filename)\n",
    "\n",
    "#load explanation\n",
    "expl_vanillagrad_nn = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52ebbe",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4c8f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn_logistic\n",
    "filename = 'explanations/expl_vanillagrad_nn_logistic.pkl'\n",
    "\n",
    "#run explanation \n",
    "expl_vanillagrad_nn_logistic = explain_vanilla_grad(model, instances, filename)\n",
    "\n",
    "#load explanation\n",
    "expl_vanillagrad_nn_logistic = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3a99e8",
   "metadata": {},
   "source": [
    "# ---------- METHOD 3: GRADIENT*INPUT ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0912fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_gradtinput(model, instances, filename):\n",
    "    model.zero_grad()\n",
    "    method = InputXGradient(model)\n",
    "    attr = method.attribute(instances)\n",
    "    \n",
    "    attr_np = attr.detach().numpy()\n",
    "    pickle.dump(attr_np, open(filename, 'wb'))\n",
    "    return attr_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60123c1",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95cefc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn\n",
    "filename = 'explanations/expl_gradtinput_nn.pkl'\n",
    "\n",
    "#run explanation\n",
    "expl_gradtinput_nn = explain_gradtinput(model, instances, filename)\n",
    "\n",
    "#load explanation\n",
    "expl_gradtinput_nn = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae09e6",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b251f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "model = model_nn_logistic\n",
    "filename = 'explanations/expl_gradtinput_nn_logistic.pkl'\n",
    "\n",
    "#run explanation\n",
    "expl_gradtinput_nn_logistic = explain_gradtinput(model, instances, filename)\n",
    "\n",
    "#load explanation\n",
    "expl_gradtinput_nn_logistic = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0088bc",
   "metadata": {},
   "source": [
    "# ---------- METHOD 4: integrated gradients ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45f6d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_integrated_grad(model, instances, n_steps=50):\n",
    "    model.zero_grad()\n",
    "    method = IntegratedGradients(model)\n",
    "    attr = method.attribute(instances, n_steps=n_steps)\n",
    "    return attr.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da3307",
   "metadata": {},
   "source": [
    "## explore convergence, NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55717bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence(model, instances, nsamples_list, filename, expl_method=['integratedgrad', 'smoothgrad']):\n",
    "    #dict to store attributions for each sample_size in nsamples_list (sample_size: attributions)\n",
    "    convergence_attr = {}\n",
    "    \n",
    "    for i in nsamples_list:\n",
    "        \n",
    "        print(f'nsamples={i}')\n",
    "        start = time.time()\n",
    "        print(f'   start: {datetime.now()}')\n",
    "        \n",
    "        #run explanation method\n",
    "        if expl_method=='integratedgrad':\n",
    "            expl_i = explain_integrated_grad(model, instances, n_steps=i)\n",
    "        elif expl_method=='smoothgrad':\n",
    "            expl_i = explain_smoothgrad(model, instances, nt_samples=i)\n",
    "        #store values\n",
    "        convergence_attr[i] = expl_i\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'   stop: {datetime.now()}')\n",
    "        print(f'   duration: {(stop-start)/60} min')\n",
    "        \n",
    "    #save data\n",
    "    pickle.dump(convergence_attr, open(filename, 'wb'))\n",
    "\n",
    "    return convergence_attr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "542cc1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn\n",
    "\n",
    "n = X_test.shape[0] #10\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "# nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "# expl_method='integratedgrad'\n",
    "# filename=f'convergence/convergence_{expl_method}_nn.pkl'\n",
    "\n",
    "# #check convergence\n",
    "# convergence_integratedgrad_nn = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "# #load file\n",
    "# convergence_integratedgrad_nn = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd403ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore convergence, nn --- plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f890d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore convergence, nn --- pick and save explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac82faa",
   "metadata": {},
   "source": [
    "## explore convergence, logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "034cd9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore convergence, logistic\n",
    "\n",
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn_logistic\n",
    "\n",
    "n = X_test.shape[0] #10\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "# nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "# expl_method='integratedgrad'\n",
    "# filename=f'convergence/convergence_{expl_method}_nn_logistic.pkl'\n",
    "\n",
    "\n",
    "\n",
    "# #check convergence\n",
    "# convergence_integratedgrad_nn_logistic = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "# #load file\n",
    "# convergence_integratedgrad_nn_logistic = pickle.load(open(filename, 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c167841",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_nn_logistic\n",
    "expl_i = explain_smoothgrad(model, instances, nt_samples=1500)\n",
    "\n",
    "pickle.dump(expl_i, open('explanations/expl_smoothgrad_nn_logistic_n1500.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa22a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3a542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58cedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c10b55b",
   "metadata": {},
   "source": [
    "# ---------- METHOD 5: smoothgrad ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5553932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_smoothgrad(model, instances, nt_samples=5):\n",
    "    model.zero_grad()\n",
    "    method = NoiseTunnel(Saliency(model))\n",
    "    attr = method.attribute(instances, nt_type='smoothgrad', nt_samples=nt_samples)\n",
    "    return attr.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8540d",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d648e44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples=50\n",
      "   start: 2021-12-14 22:38:20.443763\n",
      "   stop: 2021-12-14 22:38:20.640198\n",
      "   duration: 0.0032739837964375815 min\n",
      "nsamples=100\n",
      "   start: 2021-12-14 22:38:20.640278\n",
      "   stop: 2021-12-14 22:38:21.080281\n",
      "   duration: 0.00733335018157959 min\n",
      "nsamples=200\n",
      "   start: 2021-12-14 22:38:21.080781\n",
      "   stop: 2021-12-14 22:38:22.596869\n",
      "   duration: 0.02526810169219971 min\n",
      "nsamples=400\n",
      "   start: 2021-12-14 22:38:22.597148\n",
      "   stop: 2021-12-14 22:38:27.896745\n",
      "   duration: 0.08832656542460124 min\n",
      "nsamples=600\n",
      "   start: 2021-12-14 22:38:27.896875\n",
      "   stop: 2021-12-14 22:38:46.835533\n",
      "   duration: 0.3156442681948344 min\n",
      "nsamples=800\n",
      "   start: 2021-12-14 22:38:46.835653\n",
      "   stop: 2021-12-14 22:39:28.971945\n",
      "   duration: 0.702271330356598 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-14 22:39:28.972205\n",
      "   stop: 2021-12-14 22:40:45.289795\n",
      "   duration: 1.2719593445460002 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-14 22:40:45.290240\n",
      "   stop: 2021-12-14 22:43:34.562798\n",
      "   duration: 2.8212087313334147 min\n"
     ]
    }
   ],
   "source": [
    "#smoothgrad\n",
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn\n",
    "\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "expl_method='smoothgrad'\n",
    "filename=f'convergence/convergence_{expl_method}_nn.pkl'\n",
    "\n",
    "#check convergence\n",
    "convergence_smoothgrad_nn = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "#load file\n",
    "convergence_smoothgrad_nn = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da398bd",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "feb95844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples=50\n",
      "   start: 2021-12-14 22:43:34.625307\n",
      "   stop: 2021-12-14 22:43:34.745562\n",
      "   duration: 0.002004249890645345 min\n",
      "nsamples=100\n",
      "   start: 2021-12-14 22:43:34.745690\n",
      "   stop: 2021-12-14 22:43:35.133826\n",
      "   duration: 0.00646888017654419 min\n",
      "nsamples=200\n",
      "   start: 2021-12-14 22:43:35.137050\n",
      "   stop: 2021-12-14 22:43:36.539881\n",
      "   duration: 0.023380466302235923 min\n",
      "nsamples=400\n",
      "   start: 2021-12-14 22:43:36.540301\n",
      "   stop: 2021-12-14 22:43:43.261272\n",
      "   duration: 0.1120161493619283 min\n",
      "nsamples=600\n",
      "   start: 2021-12-14 22:43:43.261397\n",
      "   stop: 2021-12-14 22:44:04.459182\n",
      "   duration: 0.3532963832219442 min\n",
      "nsamples=800\n",
      "   start: 2021-12-14 22:44:04.459325\n",
      "   stop: 2021-12-14 22:44:50.362570\n",
      "   duration: 0.7650536696116129 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-14 22:44:50.363019\n",
      "   stop: 2021-12-14 22:46:08.352017\n",
      "   duration: 1.299816115697225 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-14 22:46:08.352683\n",
      "   stop: 2021-12-14 22:48:54.670974\n",
      "   duration: 2.7719712018966676 min\n"
     ]
    }
   ],
   "source": [
    "#smoothgrad\n",
    "#explore convergence, nn --- try different n_steps\n",
    "\n",
    "#parameters\n",
    "model = model_nn_logistic\n",
    "\n",
    "n = X_test.shape[0]\n",
    "instances = X_test.values[0:n, :]\n",
    "instances = torch.FloatTensor(instances)\n",
    "\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500]\n",
    "expl_method='smoothgrad'\n",
    "filename=f'convergence/convergence_{expl_method}_nn_logistic.pkl'\n",
    "\n",
    "#check convergence\n",
    "convergence_smoothgrad_nn_logistic = check_convergence(model, instances, nsamples_list, filename, expl_method) #!!!\n",
    "\n",
    "#load file\n",
    "convergence_smoothgrad_nn_logistic = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f757f",
   "metadata": {},
   "source": [
    "# ---------- METHOD 6: lime ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22506753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['status', 'duration', 'credit-history'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = ['status','credit-history','purpose','savings',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3cd36e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 4 3]\n",
      "[15 24 60  6 18 30 36 12  4 11 13 22 20 21 10 48 16  8  9 27 14 39 47  7\n",
      " 40 45 42 28 33  5 54]\n",
      "[3 1 2 0 4]\n",
      "[ 4  1  7  3  2  0 10  6  5  9]\n",
      "[ 1213  2439 10366  1047  1882  3441  2323  1858  2022  1503  1829  2278\n",
      "  2064  3556  3939  3535  4463  2101  3213  2675  3496  4151  5711  7057\n",
      "  1358  2150  1953  2241   709  1505  2210  1820  6850  8358  3380  6527\n",
      "  1175  1237  2394  2348  1455  7511  1597  4591   654  2002  1743   731\n",
      "  8318  1381  1169  1797  1901  1864  2122  2100  6314  2012  4057  1285\n",
      "  3872  3749  1343  1126  3652  3357  2080  5848  2327   719   931  2923\n",
      "  1236  2333  1209  1410  1048  1244  2051  2116  1262  3384  1155  1817\n",
      "  3990  6419  1360  4795  2299 12389  3573   362  2759  2872  2603  1471\n",
      "  2679  3660  2611   250  3914  1414  4272   802  1203  5433   448  6999\n",
      "  2279  2782 12579  5150  6887  3249  2993  1190  2406  6331   936  3972\n",
      "  5152  1478   888   691  6288   683  7855  6872  4280  1795   783   866\n",
      "  3021  6148   960  1940   707  1453  2118  1424  6229  4933  2835  4455\n",
      " 12749  3049  3074  2424  2753  1552  3123  1967   428  4110  1344   886\n",
      "  6615  1800  1295  2767  2214   836   874  2570 10222  2032  2978  2697\n",
      "  2225  2848  3959  1965  1366  1199 14896  3447  2831 10722  3949  1935\n",
      " 14421  1346  1526  3832  1449  3386  5179  1224  1534  7119  1569  4297\n",
      "  3578  2746  3590  6313  1282  1382  2389  3835  2171  1533  1941  2712\n",
      "  5771  1437  1555  2303  1979  8947   697  1361 12680  1258  2235  9566\n",
      "  4657  3160  8588  3931  1657  1928  1474   426 12976  4583  1808   975\n",
      "  1101   454   846  4454  1050  4716  3148  3349  1919  5998  2483  3114\n",
      "  2899  1927  3001  2762   727   932  2779  1231   902  4611  3017  1655\n",
      "  2662  2507  1963  1585  3777  1264  1750 11054   700   750  3029  3104\n",
      "  1595  5248  4576  2442  3399  6681  2359  7409  1546   759  1024  1602\n",
      "  2503  6403  2315  2384  1393  1532  3343  1980  3643  3757  2825  1275\n",
      "  2613  1577  4165   776  1313  1842  4370  6468  3878  1715   976  1413\n",
      "  1042  2146  1924  9960  1768  6078 18424  3499  4473  5742  1592  1537\n",
      "  2743  1922  1905  2578  9034  4686  1860  8133  4870  3077  1402  3913\n",
      " 12169  3051  2063  6143  6416  5743  4605  7763  1898   585  1352  2538\n",
      "  1553   795  2577  1103 11998  3446  3105  4811  1554  2775  1469  4817\n",
      "  1568  3186  1620  1961 11328  1647  1316  3414  1318  9157  2930   368\n",
      " 11816  1778  3804  4380  2320  7253  6204  1514  1164  3488  2301  1747\n",
      "  1154  1206  5842  1355  1881  2247  4594  3422   625  5951  5381  1299\n",
      "  1659  3161  1473  1322  1512   745  1493  2124   951  6967  2892  3448\n",
      "  2728  2136  2366  2238  3568  1543  1938  3656  4675  7485  1323  7408\n",
      "   790  3850  1308  2273  2302  1038 10623  1297  2404  1422  3368  7472\n",
      "  4439  3509  1740  1217  1934  2896  3244  1245  5084   958   609  2212\n",
      "  2132  8858  7166  7127  3275  1007  2058   753 14318  1204  3394  2622\n",
      "  2390  1364  5511   841  1082  5507 10875  1386  1837  1372   618  1287\n",
      "  1480  1823  3612  1520  1391 12204  9572  6614   522  1501   409  2221\n",
      "  2445  3345  2326  9629  1670  2687  2788   884  1107  3190  4153  2169\n",
      "  1459  2718  9398  1136   766   754  2337  2108  2670  9277  3234  1200\n",
      "  4241  8978  3059  1721   983   682  3108  1188  3398  7174  2799  3342\n",
      "  1680  1207   900  4843  2096  3566  1255  2329  6758  7629   781   385\n",
      "  2629   640  7418  6761  3905  1301  2520  1925  7685  8648  2569  2647\n",
      "   760  1957   939   672  8229   652  2996 11560  1567  1388  1819  6224\n",
      "  2625  1371   797  3565  1418   276  2134  3430  2069  6260   684  2249\n",
      "  1028   909  2133  7678  3016  5800   926  1495  7374  1936  3650 10974\n",
      "  2284  1442  9436  6948  8065  2462   708  5804  3149  2957  4281  2476\n",
      "  4006  1603  4712   629  5943  2671  1221  2760  1559 10127  3331 14027\n",
      "  1538  1108  1123   930  1238  7228   484  1913  2246  3763  3092  2631\n",
      "   907  3485  2142  1053  3609  1158  1271   601  5045  1412  3062  1851\n",
      "  2684  7476   660  6568  1516  1309  1216  1582  3527  3758  8386  2186\n",
      "  2859  1163 11760  4788  5103  3378   338  5801   959  1333  2751  5371\n",
      "  1283  6560   937  3857  1409 10144  5493  4746  2255  1403  6304   666\n",
      "  2319  5866  2375   947  1347  1444  4526  1225 15945  1498  1246  6742\n",
      "  1249  2149  3577  1984  4221   701  1168   730  1845   999  4351  1950\n",
      "  1037  4139   433  3868  3595  4249  6289   918  2969  8086  1201  8335\n",
      "  5003  3812  3711  2910  2748  1977  2745  1525  2580  1185  1300  3031\n",
      "  4113   763  2522  3966  9271  3863  7980   929  2346  1331  3915  1223\n",
      "  5190   860  3599  2197   392  1113  4771  2427  2964  7432   590  2463\n",
      "  6187  3651  1289  2181  1055  2812  7308  2141  1521  1337  1274  3518\n",
      "  9283  3512  9055  3594  5965  2039  3617  1880  1943  1092  1240  6361\n",
      "  6110  5954]\n",
      "[4 2 3 1 5]\n",
      "[5 2 3 4 1]\n",
      "[4 2 3 1]\n",
      "[5 2 3 1]\n",
      "[1 2 3]\n",
      "[3 4 1 2]\n",
      "[3 4 2 1]\n",
      "[47 35 42 50 25 21 24 22 37 46 28 34 29 40 26 23 38 36 61 57 32 30 68 51\n",
      " 54 48 67 39 27 43 41 33 44 31 52 55 60 53 45 20 49 75 66 64 63 19 62 58\n",
      " 59 74 65 56 70]\n",
      "[2 3 1]\n",
      "[3 2 1]\n",
      "[1 2 3 4]\n",
      "[3 4 2 1]\n",
      "[1 2]\n",
      "[2 1]\n",
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "for col in X_train:\n",
    "    print(X_train[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cb5d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_lime(predict_fn, instances, training_data, cat_idxs, n_samples, filename,\n",
    "                 mode=['classification', 'regression'], seed=12345):\n",
    "    #create lime explainer\n",
    "    np.random.seed(seed)\n",
    "    explainer = lime_tabular.LimeTabularExplainer(training_data=training_data, \n",
    "                                                  mode=mode, \n",
    "                                                  discretize_continuous=False,\n",
    "                                                  sample_around_instance=True,\n",
    "                                                  random_state=seed, \n",
    "                                                  categorical_features=cat_idxs)\n",
    "    #explain each data point in 'instances'\n",
    "    exps = []\n",
    "    num_feat = instances.shape[1]\n",
    "    for x in instances:\n",
    "        exp = explainer.explain_instance(x, \n",
    "                                         predict_fn=predict_fn,\n",
    "                                         num_samples=n_samples,\n",
    "                                         num_features=num_feat).local_exp[1]\n",
    "        #format explanations\n",
    "        exp = sorted(exp, key=lambda tup: tup[0])\n",
    "        exp = [t[1] for t in exp]\n",
    "        exps.append(exp)\n",
    "    \n",
    "    #save explanations\n",
    "    exps = np.array(exps)\n",
    "    pickle.dump(exps, open(filename, 'wb'))\n",
    "    \n",
    "    return exps #n x p (same dimensions as 'instances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e10d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence_lime(predict_fn, instances, training_data, cat_idxs, filename_lime, mode,\n",
    "                           nsamples_list, filename_convergence):\n",
    "    #dict to store attributions for each sample_size in nsamples_list (sample_size: attributions)\n",
    "    convergence_attr = {}\n",
    "    \n",
    "    for i in nsamples_list:\n",
    "        \n",
    "        print(f'nsamples={i}')\n",
    "        start = time.time()\n",
    "        print(f'   start: {datetime.now()}')\n",
    "        \n",
    "        #run explanation method\n",
    "        expl_i = explain_lime(predict_fn, instances, training_data, cat_idxs, \n",
    "                              n_samples=i, filename=f'{filename_lime}_n{i}.pkl', mode=mode)\n",
    "        #store values\n",
    "        convergence_attr[i] = expl_i\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'   stop: {datetime.now()}')\n",
    "        print(f'   duration: {(stop-start)/60} min')\n",
    "        \n",
    "    #save data\n",
    "    pickle.dump(convergence_attr, open(filename_convergence, 'wb'))\n",
    "\n",
    "    return convergence_attr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57c6028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = np.arange(0,20).tolist()\n",
    "new_list.remove(1)\n",
    "new_list.remove(4)\n",
    "new_list.remove(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86197943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionaries\n",
    "model_names = ['logistic', 'gb', 'rf', 'nn', 'nn_logistic']\n",
    "predict_fns_2classes = {'logistic': model_logistic.predict_proba, \n",
    "               'gb': model_gb.predict_proba, \n",
    "               'rf': model_rf.predict_proba, \n",
    "               'nn': model_nn.predict_proba,\n",
    "               'nn_logistic': model_nn_logistic.predict_proba}\n",
    "\n",
    "\n",
    "#arguments that are constant\n",
    "n = X_test.shape[0] #10\n",
    "instances = X_test.values[0:n, :]\n",
    "training_data = X_train.values\n",
    "\n",
    "cat_idxs = new_list\n",
    "\n",
    "mode = 'classification'\n",
    "nsamples_list = [50, 100, 200, 400, 600, 800, 1000, 1500, 2000, 2500, 3000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79e2308e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******convergence analysis for logistic******\n",
      "nsamples=50\n",
      "   start: 2021-12-19 23:44:56.519704\n",
      "   stop: 2021-12-19 23:44:56.917051\n",
      "   duration: 0.006622485319773356 min\n",
      "nsamples=100\n",
      "   start: 2021-12-19 23:44:56.917159\n",
      "   stop: 2021-12-19 23:44:57.274058\n",
      "   duration: 0.005948301156361898 min\n",
      "nsamples=200\n",
      "   start: 2021-12-19 23:44:57.274155\n",
      "   stop: 2021-12-19 23:44:57.657286\n",
      "   duration: 0.006385505199432373 min\n",
      "nsamples=400\n",
      "   start: 2021-12-19 23:44:57.657360\n",
      "   stop: 2021-12-19 23:44:58.143807\n",
      "   duration: 0.008107419808705647 min\n",
      "nsamples=600\n",
      "   start: 2021-12-19 23:44:58.143918\n",
      "   stop: 2021-12-19 23:44:58.710826\n",
      "   duration: 0.00944844881693522 min\n",
      "nsamples=800\n",
      "   start: 2021-12-19 23:44:58.711082\n",
      "   stop: 2021-12-19 23:44:59.338292\n",
      "   duration: 0.010453482468922934 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-19 23:44:59.338435\n",
      "   stop: 2021-12-19 23:45:00.034789\n",
      "   duration: 0.011605886618296306 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-19 23:45:00.034888\n",
      "   stop: 2021-12-19 23:45:00.863493\n",
      "   duration: 0.013810050487518311 min\n",
      "nsamples=2000\n",
      "   start: 2021-12-19 23:45:00.863696\n",
      "   stop: 2021-12-19 23:45:01.896642\n",
      "   duration: 0.01721574862798055 min\n",
      "nsamples=2500\n",
      "   start: 2021-12-19 23:45:01.896791\n",
      "   stop: 2021-12-19 23:45:03.112201\n",
      "   duration: 0.020256797472635906 min\n",
      "nsamples=3000\n",
      "   start: 2021-12-19 23:45:03.112333\n",
      "   stop: 2021-12-19 23:45:04.595339\n",
      "   duration: 0.02471673091252645 min\n",
      "******convergence analysis for gb******\n",
      "nsamples=50\n",
      "   start: 2021-12-19 23:45:04.596380\n",
      "   stop: 2021-12-19 23:45:05.000730\n",
      "   duration: 0.006739171346028646 min\n",
      "nsamples=100\n",
      "   start: 2021-12-19 23:45:05.000822\n",
      "   stop: 2021-12-19 23:45:05.390682\n",
      "   duration: 0.006497649351755778 min\n",
      "nsamples=200\n",
      "   start: 2021-12-19 23:45:05.390922\n",
      "   stop: 2021-12-19 23:45:05.808212\n",
      "   duration: 0.006954832871754964 min\n",
      "nsamples=400\n",
      "   start: 2021-12-19 23:45:05.808293\n",
      "   stop: 2021-12-19 23:45:06.378943\n",
      "   duration: 0.00951079527537028 min\n",
      "nsamples=600\n",
      "   start: 2021-12-19 23:45:06.379060\n",
      "   stop: 2021-12-19 23:45:07.042468\n",
      "   duration: 0.011056768894195556 min\n",
      "nsamples=800\n",
      "   start: 2021-12-19 23:45:07.042609\n",
      "   stop: 2021-12-19 23:45:07.773808\n",
      "   duration: 0.01218663454055786 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-19 23:45:07.773914\n",
      "   stop: 2021-12-19 23:45:08.570358\n",
      "   duration: 0.013274049758911133 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-19 23:45:08.570658\n",
      "   stop: 2021-12-19 23:45:09.661147\n",
      "   duration: 0.01817479928334554 min\n",
      "nsamples=2000\n",
      "   start: 2021-12-19 23:45:09.661282\n",
      "   stop: 2021-12-19 23:45:10.904624\n",
      "   duration: 0.020722365379333495 min\n",
      "nsamples=2500\n",
      "   start: 2021-12-19 23:45:10.904783\n",
      "   stop: 2021-12-19 23:45:12.311208\n",
      "   duration: 0.02344036897023519 min\n",
      "nsamples=3000\n",
      "   start: 2021-12-19 23:45:12.311418\n",
      "   stop: 2021-12-19 23:45:13.891370\n",
      "   duration: 0.026332529385884602 min\n",
      "******convergence analysis for rf******\n",
      "nsamples=50\n",
      "   start: 2021-12-19 23:45:13.892325\n",
      "   stop: 2021-12-19 23:45:14.740917\n",
      "   duration: 0.01414319674173991 min\n",
      "nsamples=100\n",
      "   start: 2021-12-19 23:45:14.741005\n",
      "   stop: 2021-12-19 23:45:15.699582\n",
      "   duration: 0.015976266066233317 min\n",
      "nsamples=200\n",
      "   start: 2021-12-19 23:45:15.699683\n",
      "   stop: 2021-12-19 23:45:16.703875\n",
      "   duration: 0.016736515363057456 min\n",
      "nsamples=400\n",
      "   start: 2021-12-19 23:45:16.703966\n",
      "   stop: 2021-12-19 23:45:18.054648\n",
      "   duration: 0.022511335213979085 min\n",
      "nsamples=600\n",
      "   start: 2021-12-19 23:45:18.054822\n",
      "   stop: 2021-12-19 23:45:19.655170\n",
      "   duration: 0.02667246659596761 min\n",
      "nsamples=800\n",
      "   start: 2021-12-19 23:45:19.655322\n",
      "   stop: 2021-12-19 23:45:21.462311\n",
      "   duration: 0.030116462707519533 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-19 23:45:21.462445\n",
      "   stop: 2021-12-19 23:45:23.462282\n",
      "   duration: 0.0333305835723877 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-19 23:45:23.462443\n",
      "   stop: 2021-12-19 23:45:26.002506\n",
      "   duration: 0.042334365844726565 min\n",
      "nsamples=2000\n",
      "   start: 2021-12-19 23:45:26.002825\n",
      "   stop: 2021-12-19 23:45:29.047964\n",
      "   duration: 0.050752286116282144 min\n",
      "nsamples=2500\n",
      "   start: 2021-12-19 23:45:29.048125\n",
      "   stop: 2021-12-19 23:45:32.715799\n",
      "   duration: 0.061127865314483644 min\n",
      "nsamples=3000\n",
      "   start: 2021-12-19 23:45:32.715914\n",
      "   stop: 2021-12-19 23:45:37.071912\n",
      "   duration: 0.07259994745254517 min\n",
      "******convergence analysis for nn******\n",
      "nsamples=50\n",
      "   start: 2021-12-19 23:45:37.072835\n",
      "   stop: 2021-12-19 23:45:37.496682\n",
      "   duration: 0.00706410010655721 min\n",
      "nsamples=100\n",
      "   start: 2021-12-19 23:45:37.496793\n",
      "   stop: 2021-12-19 23:45:37.898817\n",
      "   duration: 0.006700380643208822 min\n",
      "nsamples=200\n",
      "   start: 2021-12-19 23:45:37.898902\n",
      "   stop: 2021-12-19 23:45:38.339638\n",
      "   duration: 0.007345585028330485 min\n",
      "nsamples=400\n",
      "   start: 2021-12-19 23:45:38.339725\n",
      "   stop: 2021-12-19 23:45:38.971893\n",
      "   duration: 0.010536114374796549 min\n",
      "nsamples=600\n",
      "   start: 2021-12-19 23:45:38.972010\n",
      "   stop: 2021-12-19 23:45:39.736535\n",
      "   duration: 0.012742050488789876 min\n",
      "nsamples=800\n",
      "   start: 2021-12-19 23:45:39.736670\n",
      "   stop: 2021-12-19 23:45:40.561538\n",
      "   duration: 0.013747767607371012 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-19 23:45:40.561775\n",
      "   stop: 2021-12-19 23:45:41.487764\n",
      "   duration: 0.015433128674825032 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-19 23:45:41.488013\n",
      "   stop: 2021-12-19 23:45:42.643147\n",
      "   duration: 0.019252216815948485 min\n",
      "nsamples=2000\n",
      "   start: 2021-12-19 23:45:42.643248\n",
      "   stop: 2021-12-19 23:45:44.027910\n",
      "   duration: 0.023077682654062907 min\n",
      "nsamples=2500\n",
      "   start: 2021-12-19 23:45:44.028017\n",
      "   stop: 2021-12-19 23:45:45.627472\n",
      "   duration: 0.026657585302988687 min\n",
      "nsamples=3000\n",
      "   start: 2021-12-19 23:45:45.627760\n",
      "   stop: 2021-12-19 23:45:47.479992\n",
      "   duration: 0.030870497226715088 min\n",
      "******convergence analysis for nn_logistic******\n",
      "nsamples=50\n",
      "   start: 2021-12-19 23:45:47.481159\n",
      "   stop: 2021-12-19 23:45:47.847094\n",
      "   duration: 0.00609892209370931 min\n",
      "nsamples=100\n",
      "   start: 2021-12-19 23:45:47.847168\n",
      "   stop: 2021-12-19 23:45:48.210092\n",
      "   duration: 0.006048715114593506 min\n",
      "nsamples=200\n",
      "   start: 2021-12-19 23:45:48.210201\n",
      "   stop: 2021-12-19 23:45:48.608874\n",
      "   duration: 0.006644531091054281 min\n",
      "nsamples=400\n",
      "   start: 2021-12-19 23:45:48.609110\n",
      "   stop: 2021-12-19 23:45:49.121943\n",
      "   duration: 0.008547218640645345 min\n",
      "nsamples=600\n",
      "   start: 2021-12-19 23:45:49.122088\n",
      "   stop: 2021-12-19 23:45:49.764800\n",
      "   duration: 0.010711864630381266 min\n",
      "nsamples=800\n",
      "   start: 2021-12-19 23:45:49.764906\n",
      "   stop: 2021-12-19 23:45:50.445620\n",
      "   duration: 0.011345199743906657 min\n",
      "nsamples=1000\n",
      "   start: 2021-12-19 23:45:50.445728\n",
      "   stop: 2021-12-19 23:45:51.177754\n",
      "   duration: 0.012200399239857992 min\n",
      "nsamples=1500\n",
      "   start: 2021-12-19 23:45:51.177904\n",
      "   stop: 2021-12-19 23:45:52.074464\n",
      "   duration: 0.014942630132039388 min\n",
      "nsamples=2000\n",
      "   start: 2021-12-19 23:45:52.074623\n",
      "   stop: 2021-12-19 23:45:53.126447\n",
      "   duration: 0.017530401547749836 min\n",
      "nsamples=2500\n",
      "   start: 2021-12-19 23:45:53.126632\n",
      "   stop: 2021-12-19 23:45:54.352130\n",
      "   duration: 0.02042495012283325 min\n",
      "nsamples=3000\n",
      "   start: 2021-12-19 23:45:54.352340\n",
      "   stop: 2021-12-19 23:45:55.659418\n",
      "   duration: 0.021784619490305582 min\n"
     ]
    }
   ],
   "source": [
    "#for each model\n",
    "for m in range(0, len(model_names)):\n",
    "    model = model_names[m]\n",
    "    print(f'******convergence analysis for {model}******')\n",
    "    \n",
    "    #run convergence analysis\n",
    "    filename_lime=f'explanations/expl_lime_{model}'\n",
    "    filename_convergence=f'convergence/convergence_lime_{model}.pkl'\n",
    "    _ = check_convergence_lime(predict_fn=predict_fns_2classes[model], \n",
    "                           instances=instances, \n",
    "                           training_data=training_data, \n",
    "                           cat_idxs=cat_idxs, \n",
    "                           filename_lime=filename_lime, \n",
    "                           mode=mode,\n",
    "                           nsamples_list=nsamples_list, \n",
    "                           filename_convergence=filename_convergence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41796b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e47e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af33ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
